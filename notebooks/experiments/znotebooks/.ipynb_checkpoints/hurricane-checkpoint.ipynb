{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "600dd37a-18f6-4efb-95d2-9018d4fe4f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: packaging in /glade/u/apps/jupyterhub/jh-23.11/lib/python3.10/site-packages (from tensorflow-addons) (23.2)\n",
      "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: typeguard, tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d1aa354-4561-4486-9f61-73f2c1c81a31",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 08:38:28.942672: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-06 08:38:28.958819: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738856308.976587   65402 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738856308.982216   65402 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-06 08:38:29.001991: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import sklearn.model_selection\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras import layers\n",
    "\n",
    "import keras_tuner\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "keras.utils.set_random_seed(812)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c64da756-7c64-4092-b106-abbb5e8bd585",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting zarr\n",
      "  Downloading zarr-2.18.3-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting asciitree (from zarr)\n",
      "  Downloading asciitree-0.3.3.tar.gz (4.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.24 in /glade/u/apps/jupyterhub/jh-23.11/lib/python3.10/site-packages (from zarr) (1.26.0)\n",
      "Collecting numcodecs>=0.10.0 (from zarr)\n",
      "  Downloading numcodecs-0.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
      "Collecting fasteners (from zarr)\n",
      "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
      "Downloading zarr-2.18.3-py3-none-any.whl (210 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m914.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numcodecs-0.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
      "Building wheels for collected packages: asciitree\n",
      "  Building wheel for asciitree (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for asciitree: filename=asciitree-0.3.3-py3-none-any.whl size=5034 sha256=f596e69d74c710c5b28b815ee15777cac5b22b767eeac353692f9dce0cf22474\n",
      "  Stored in directory: /glade/u/home/rmandava/.cache/pip/wheels/7f/4e/be/1171b40f43b918087657ec57cf3b81fa1a2e027d8755baa184\n",
      "Successfully built asciitree\n",
      "Installing collected packages: asciitree, numcodecs, fasteners, zarr\n",
      "Successfully installed asciitree-0.3.3 fasteners-0.19 numcodecs-0.13.1 zarr-2.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install zarr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e35bf81-6ef9-45ef-bd0a-0cb2523ec15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USER OPTIONS\n",
    "\n",
    "var_list = [\"cape\", \"crr\", \"d\", \"ie\", \"ishf\", \"lsrr\", \"pv\", \"q\", \"r\", \"sp\", \"sstk\", \"tcw\", \"tcwv\", \"t\",  \"ttr\", \"u\",  \"v\", \"vo\", \"w\"] #ERA5 variables\n",
    "\n",
    "aew_subset = \"12hr_before\" #time of interest\n",
    "\n",
    "plevel_list = [False, False, 300, False, False, False, 300, 300, 300, False, False, False, False, 300,  False, 300,  300,  300, 300] #pressure levels of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c74c2082-8917-4d0d-a2d7-b39f8548bcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to calculate f1 score as loss function\n",
    "\n",
    "def f1_loss_sigmoid(y_true, y_pred):\n",
    "    \"\"\"\n",
    "\n",
    "    F1 metric for sigmoid output and integer encoded labels.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # compute tp, fp, and fn\n",
    "\n",
    "    tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n",
    "\n",
    "    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\n",
    "\n",
    "    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\n",
    "\n",
    "\n",
    "    # precision (tp / (tp + fp))\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "\n",
    "   # recall (tp / (tp + fn))\n",
    "\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "\n",
    "# harmonic mean of precision and recall\n",
    "\n",
    "    f1 = (2 * p * r) / (p + r + K.epsilon())\n",
    "\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "\n",
    "    return 1 - K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58fe83a8-b5b0-4a10-81f6-a5b9277e5a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_loss_onehot(y_true, y_pred):\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "   F1 metric for two-class output and one-hot encoded labels.\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "   # compute tp, fp, and fn\n",
    "\n",
    "   tp = K.sum(K.cast(y_true[:, 1] * y_pred[:, 1], 'float'), axis=0)\n",
    "\n",
    "   fp = K.sum(K.cast((1 - y_true[:, 1]) * y_pred[:, 1], 'float'), axis=0)\n",
    "\n",
    "   fn = K.sum(K.cast(y_true[:, 0] * (1 - y_pred[:, 0]), 'float'), axis=0)\n",
    "\n",
    "\n",
    "   # precision (tp / (tp + fp))\n",
    "\n",
    "   p = tp / (tp + fp + K.epsilon())\n",
    "\n",
    "   # recall (tp / (tp + fn))\n",
    "\n",
    "   r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "\n",
    "   # harmonic mean of precision and recall\n",
    "\n",
    "   f1 = (2 * p * r) / (p + r + K.epsilon())\n",
    "\n",
    "   f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "\n",
    "   return 1 - K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82e6df90-377d-4c27-ac1d-156b3bc34623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dim(ds):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Preprocessing help when opening netcdf files\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return ds.assign_coords({\"sample\": 1}).expand_dims(dim={\"sample\": 1}).drop_vars(\n",
    "\n",
    "    'utc_date').drop_vars(\"latitude\").drop_vars('longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec3de133-3ed1-4df3-9444-a58e88716284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "def open_files_zarr(list_of_vars, aew_subset=\"12hr_before\",\n",
    "                    directory=\"/glade/derecho/scratch/rmandava/AEW_time_location_files/\",\n",
    "                    plevel_list=None, zarr_store_path=\"zarr_data\"):\n",
    "    \"\"\"\n",
    "    Opens ERA5 NetCDF files for the given variables. For each variable (or pressure-level variant),\n",
    "    it checks if a corresponding Zarr store exists in 'zarr_store_path'. If so, it loads the dataset\n",
    "    from the Zarr store; if not, it opens the NetCDF files, preprocesses them, saves them to Zarr,\n",
    "    and then returns the dataset.\n",
    "    \"\"\"\n",
    "    # Create the zarr_store_path directory if it doesn't exist.\n",
    "    if not os.path.exists(zarr_store_path):\n",
    "        os.makedirs(zarr_store_path)\n",
    "    \n",
    "    datas = {}\n",
    "    for num, var in enumerate(list_of_vars):\n",
    "        # Determine the key and filename based on whether a pressure level is specified.\n",
    "        if plevel_list:\n",
    "            if plevel_list[num]:\n",
    "                key = f\"{var}_{int(plevel_list[num])}\"\n",
    "                file_pattern = f'{directory}/{var}/aew_{aew_subset}_{int(plevel_list[num])}_*.nc'\n",
    "            else:\n",
    "                key = var\n",
    "                file_pattern = f'{directory}/{var}/aew_{aew_subset}_*.nc'\n",
    "        else:\n",
    "            key = var\n",
    "            file_pattern = f'{directory}/{var}/aew_{aew_subset}_*.nc'\n",
    "        \n",
    "        # Define the zarr path for this variable.\n",
    "        zarr_path = os.path.join(zarr_store_path, f\"{key}.zarr\")\n",
    "        \n",
    "        # If the Zarr dataset exists, load from it; otherwise, create it.\n",
    "        if os.path.exists(zarr_path):\n",
    "            print(f\"Loading {key} from Zarr store.\")\n",
    "            ds = xr.open_zarr(zarr_path)\n",
    "        else:\n",
    "            print(f\"Creating Zarr store for {key} from NetCDF files.\")\n",
    "            ds = xr.open_mfdataset(\n",
    "                file_pattern,\n",
    "                preprocess=add_dim,\n",
    "                concat_dim=\"sample\",\n",
    "                combine=\"nested\",\n",
    "            )\n",
    "            ds.to_zarr(zarr_path, mode=\"w\")\n",
    "        datas[key] = ds\n",
    "    \n",
    "    return datas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c20c80e6-2437-4626-be0b-4d82bebb4f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_load_concat(data_dictionary):\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "   Eagerly load the training labels and data, reshaping data to (samples, y, x, features).\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "   thedatas = {}\n",
    "\n",
    "\n",
    "   for key, value in data_dictionary.items():\n",
    "\n",
    "\n",
    "       if '_' not in key:\n",
    "\n",
    "\n",
    "          thedatas[key] = value[key.upper()].expand_dims('features').transpose(\n",
    "\n",
    "                           'sample', 'latitude', 'longitude', 'features').values\n",
    "\n",
    "\n",
    "       if '_' in key:\n",
    "\n",
    "\n",
    "          thedatas[key] = value[key.split('_')[0].upper()].expand_dims('features').transpose(\n",
    "\n",
    "                          'sample', 'latitude', 'longitude', 'features').values\n",
    "\n",
    "\n",
    "       label = value['label'].values\n",
    "\n",
    "\n",
    "   if len(data_dictionary) > 1:\n",
    "\n",
    "       data = np.concatenate(list(thedatas.values()), axis=3)\n",
    "   if len(data_dictionary) == 1:\n",
    "\n",
    "       data = np.squeeze(np.asarray(list(thedatas.values())), axis=0)\n",
    "   return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bec2717c-8176-447f-82ea-5b15e038ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def omit_nans(data, label):\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "   Remove any ``nans`` from the data.\n",
    "\n",
    "   Args:\n",
    "\n",
    "   data (numpy array): Training data.\n",
    "\n",
    "   label (numpy array): Labels for supervised learning.\n",
    "\n",
    "   Returns:\n",
    "\n",
    "   data (numpy array): Training data with ``nans`` removed.\n",
    "\n",
    "   label (numpy array): Corresponding labels of data.\n",
    " \n",
    "   \"\"\"\n",
    "\n",
    "   maskarray = np.full(data.shape[0], True)\n",
    "\n",
    "   masker = np.unique(np.argwhere(np.isnan(data))[:, 0])\n",
    "\n",
    "   maskarray[masker] = False\n",
    "\n",
    "   traindata = data[maskarray, :, :, :]\n",
    "\n",
    "   trainlabel = label[maskarray]\n",
    "\n",
    "   return traindata, trainlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d02dbe1a-f1ba-4f9b-8fe0-d9126814ae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(data):\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  Rescaling the data using zscore (mean/std).\n",
    "\n",
    "  Each variable gets scaled independently from others.\n",
    "\n",
    "  Note that we will need to remove test data for formal training.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  for i in range(0, data_.shape[-1]):\n",
    "\n",
    "      data_[:, :, :, i] = (\n",
    "\n",
    "               data_[:, :, :, i] - np.nanmean(\n",
    "\n",
    "                     data_[:, :, :, i])) / np.nanstd(data_[:, :, :, i])\n",
    "\n",
    "  return data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "332bc130-a4e7-465c-a135-687f543db181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax(data):\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "   Rescaling the data using min-max.\n",
    "\n",
    "   Each variable gets scaled independently from others.\n",
    "\n",
    "   Note that we will need to remove test data for formal training.\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "   for i in range(0, data_.shape[-1]):\n",
    "\n",
    "          data_[:, :, :, i] = (\n",
    "\n",
    "              data_[:, :, :, i] - np.nanmin(data_[:, :, :, i])\n",
    "\n",
    "          ) / (np.nanmax(data_[:, :, :, i]) - np.nanmin(data_[:, :, :, i]))\n",
    "   return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f652c778-f81b-4c46-bf87-d60487903b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(data, label, split=0.3, seed=0):\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "   Help spliting data randomly for training and testing.\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "   np.random.seed(0)\n",
    "\n",
    "   da_indx = np.random.permutation(data.shape[0])\n",
    "\n",
    "   data = data[da_indx.astype(int)]\n",
    "\n",
    "   label = label[da_indx.astype(int)]\n",
    "\n",
    "   init_range = int(data.shape[0] * (1 - 0.3))\n",
    "\n",
    "   return data[:init_range], label[:init_range], data[init_range:], label[init_range:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9f970b2-2444-4dba-8e66-a850482d5ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_loss(loss_string):\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "   Help selecting the activation functions\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "   lossdict = {\n",
    "\n",
    "           \"relu\": keras.activations.relu,\n",
    "\n",
    "           \"tanh\": keras.activations.tanh,\n",
    "\n",
    "           \"selu\": keras.activations.selu,\n",
    "\n",
    "           \"sigmoid\": keras.activations.sigmoid,\n",
    "\n",
    "           \"relu6\": keras.activations.relu6,\n",
    "\n",
    "           \"silu\": keras.activations.silu,\n",
    "\n",
    "           \"gelu\": keras.activations.gelu,\n",
    "\n",
    "           \"lrelu\": keras.activations.leaky_relu,\n",
    "\n",
    "   }\n",
    "\n",
    "   return lossdict[loss_string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81aa5f1b-5513-4496-9fae-a6cde06e1aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_features = len(var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3490622e-a626-42aa-9cfe-020ee54d21e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cape from Zarr store.\n",
      "Loading crr from Zarr store.\n",
      "Loading d_300 from Zarr store.\n",
      "Loading ie from Zarr store.\n",
      "Loading ishf from Zarr store.\n",
      "Loading lsrr from Zarr store.\n",
      "Loading pv_300 from Zarr store.\n",
      "Loading q_300 from Zarr store.\n",
      "Loading r_300 from Zarr store.\n",
      "Loading sp from Zarr store.\n",
      "Loading sstk from Zarr store.\n",
      "Loading tcw from Zarr store.\n",
      "Loading tcwv from Zarr store.\n",
      "Loading t_300 from Zarr store.\n",
      "Loading ttr from Zarr store.\n",
      "Loading u_300 from Zarr store.\n",
      "Loading v_300 from Zarr store.\n",
      "Loading vo_300 from Zarr store.\n",
      "Loading w_300 from Zarr store.\n"
     ]
    }
   ],
   "source": [
    "data = open_files_zarr(\n",
    "    list_of_vars=var_list,\n",
    "    aew_subset=aew_subset,\n",
    "    directory=\"/glade/derecho/scratch/rmandava/AEW_time_location_files/\",\n",
    "    plevel_list=plevel_list,\n",
    "    zarr_store_path=\"/glade/derecho/scratch/rmandava/AEW_time_location_files/project/zarr\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd0f25a9-ec43-4e96-bb26-9a70821f593d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sample  latitude  longitude      CAPE  label                time\n",
      "0       1         0          0  2114.125    0.0 1979-06-02 18:00:00\n",
      "1       1         0          1  1550.375    0.0 1979-06-02 18:00:00\n",
      "2       1         0          2  1955.875    0.0 1979-06-02 18:00:00\n",
      "3       1         0          3  1876.500    0.0 1979-06-02 18:00:00\n",
      "4       1         0          4  1666.750    0.0 1979-06-02 18:00:00\n"
     ]
    }
   ],
   "source": [
    "ds_cape = data[\"cape\"]\n",
    "df_cape = ds_cape.to_dataframe().reset_index()\n",
    "print(df_cape.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc9df096-2cf6-4585-960f-641627731c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2816000 entries, 0 to 2815999\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Dtype         \n",
      "---  ------     -----         \n",
      " 0   sample     int64         \n",
      " 1   latitude   int64         \n",
      " 2   longitude  int64         \n",
      " 3   CAPE       float32       \n",
      " 4   label      float64       \n",
      " 5   time       datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float32(1), float64(1), int64(3)\n",
      "memory usage: 118.2 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_cape.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3a24d36-9e8a-43dc-b831-39c3149437fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sample  latitude  longitude         U  label  level                time\n",
      "0       1         0          0 -2.783066    0.0  300.0 1979-06-02 18:00:00\n",
      "1       1         0          1 -3.501816    0.0  300.0 1979-06-02 18:00:00\n",
      "2       1         0          2 -3.951035    0.0  300.0 1979-06-02 18:00:00\n",
      "3       1         0          3 -4.279160    0.0  300.0 1979-06-02 18:00:00\n",
      "4       1         0          4 -4.380722    0.0  300.0 1979-06-02 18:00:00\n"
     ]
    }
   ],
   "source": [
    "ds_u = data[\"u_300\"]\n",
    "df_u = ds_u.to_dataframe().reset_index()\n",
    "print(df_u.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91980400-17e1-4ba2-93a7-9233c67c0003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2816000 entries, 0 to 2815999\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Dtype         \n",
      "---  ------     -----         \n",
      " 0   sample     int64         \n",
      " 1   latitude   int64         \n",
      " 2   longitude  int64         \n",
      " 3   U          float32       \n",
      " 4   label      float64       \n",
      " 5   level      float64       \n",
      " 6   time       datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float32(1), float64(2), int64(3)\n",
      "memory usage: 139.6 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_u.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f240d4af-46d1-4cef-a5ab-2e2527a1639f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2750, 32, 32, 19)\n"
     ]
    }
   ],
   "source": [
    "# transpose the data and concat variables\n",
    "\n",
    "data_, labels_ = transpose_load_concat(data)\n",
    "\n",
    "print (np.shape(data_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0e4f7b8-f9e0-4131-ae35-97e47bc557d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check / remove nans\n",
    "\n",
    "data_, labels_ = omit_nans(data_, labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0492935e-1f35-446b-ab16-aba0e667857b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(560, 32, 32, 19) (140, 32, 32, 19) (560,) (140,)\n"
     ]
    }
   ],
   "source": [
    "#split train and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "\n",
    "data_, labels_, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print (np.shape(X_train), np.shape(X_test), np.shape(y_train), np.shape(y_test))\n",
    "\n",
    "\n",
    "y_train = np.expand_dims(y_train, axis=1)\n",
    "\n",
    "y_test = np.expand_dims(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "164ee316-7c83-474b-bdb9-7c12ca165e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now rescale using sklearn object\n",
    "\n",
    "# create our scaler object\n",
    "\n",
    "# scaler_input = sklearn.preprocessing.MinMaxScaler()\n",
    "\n",
    "#Zscore scaling\n",
    "\n",
    "scaler_input = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "\n",
    "# now fit and transform our input data and labels\n",
    "\n",
    "# training data\n",
    "\n",
    "# inputs\n",
    "\n",
    "X_train_tmp = np.reshape(X_train, (-1, len(var_list)))\n",
    "\n",
    "input_train_scaled = scaler_input.fit_transform(X_train_tmp)\n",
    "\n",
    "input_train_scaled = np.reshape(input_train_scaled, X_train.shape)\n",
    "\n",
    "# labels\n",
    "\n",
    "label_train_scaled = y_train\n",
    "\n",
    "\n",
    "# testing data\n",
    "\n",
    "# inputs\n",
    "\n",
    "X_test_tmp = np.reshape(X_test, (-1, len(var_list)))\n",
    "\n",
    "input_test_scaled = scaler_input.fit_transform(X_test_tmp)\n",
    "\n",
    "input_test_scaled = np.reshape(input_test_scaled, X_test.shape)\n",
    "\n",
    "# labels\n",
    "\n",
    "label_test_scaled = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bed418e-264b-45e6-92ee-b72c2f7a04d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(560, 32, 32, 19) (560, 1) (140, 32, 32, 19) (140, 1)\n"
     ]
    }
   ],
   "source": [
    "# print the shapes to double check them\n",
    "\n",
    "print(\n",
    "\n",
    "input_train_scaled.shape,\n",
    "\n",
    "label_train_scaled.shape,\n",
    "\n",
    "input_test_scaled.shape,\n",
    "\n",
    "label_test_scaled.shape\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10597eb7-f8fa-4523-a306-55d5a442d504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive samples in training data: 91 (16.25% of total)\n"
     ]
    }
   ],
   "source": [
    "#generate class weights due to class imbalance issues\n",
    "\n",
    "counts = np.bincount(y_train[:, 0].astype(int))\n",
    "\n",
    "\n",
    "print(\n",
    "\n",
    "\"Number of positive samples in training data: {} ({:.2f}% of total)\".format(\n",
    "\n",
    "counts[1], 100 * float(counts[1]) / len(y_train))\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "557d98a2-07ba-4fa7-8ba2-9ce8baa53b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.1625, 1: 0.8375}\n"
     ]
    }
   ],
   "source": [
    "# old weights\n",
    "\n",
    "# weight_for_0 = 1.0 / counts[0]\n",
    "\n",
    "# weight_for_1 = 1.0 / counts[1]\n",
    "\n",
    "\n",
    "#new weights\n",
    "\n",
    "weight_for_0 = float(counts[1]) / len(y_train)\n",
    "\n",
    "weight_for_1 = 1 - (float(counts[1]) / len(y_train))\n",
    "\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "700f978d-cc6f-484a-864f-dcb9e703eed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 08:39:54.692778: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "METRICS = [\n",
    "\n",
    "keras.metrics.BinaryCrossentropy(name='cross entropy'),\n",
    "\n",
    "keras.metrics.MeanSquaredError(name='mean_squared_error'),\n",
    "\n",
    "keras.metrics.RootMeanSquaredError(name='root_mean_squared_error'),\n",
    "\n",
    "keras.metrics.TruePositives(name='tp'),\n",
    "\n",
    "keras.metrics.FalsePositives(name='fp'),\n",
    "\n",
    "keras.metrics.TrueNegatives(name='tn'),\n",
    "\n",
    "keras.metrics.FalseNegatives(name='fn'),\n",
    "\n",
    "keras.metrics.BinaryAccuracy(name='binary_accuracy'),\n",
    "\n",
    "keras.metrics.F1Score(threshold=0.5, name='f1_score'),\n",
    "\n",
    "keras.metrics.Precision(name='precision'),\n",
    "\n",
    "keras.metrics.Recall(name='recall'),\n",
    "\n",
    "keras.metrics.AUC(name='auc'),\n",
    "\n",
    "keras.metrics.AUC(name='prc', curve='PR'),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc7f9c27-6372-4ee3-975a-affdd7a992d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    \"\"\"\n",
    "    Compute focal loss for binary classification.\n",
    "    Args:\n",
    "      gamma: focusing parameter for modulating factor (1-p)\n",
    "      alpha: balance parameter for class weights.\n",
    "    Returns:\n",
    "      A loss function that computes the focal loss.\n",
    "    \"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        epsilon = 1e-7  # small constant to avoid log(0)\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        # Compute cross-entropy\n",
    "        cross_entropy = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n",
    "        # Compute focal weight: scale loss for hard examples more\n",
    "        weight = alpha * tf.pow(1 - y_pred, gamma) * y_true + (1 - alpha) * tf.pow(y_pred, gamma) * (1 - y_true)\n",
    "        loss_val = weight * cross_entropy\n",
    "        return tf.reduce_mean(loss_val)\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6bc9aab7-2558-4304-a6c7-5809d60e8437",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyImprovedHyperModel(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = keras.Sequential()\n",
    "        # Input layer: expecting 32x32 images with 'number_of_features' channels.\n",
    "        model.add(keras.Input(shape=(32, 32, number_of_features)))\n",
    "        \n",
    "        # Data augmentation layers\n",
    "        model.add(layers.RandomFlip(\"horizontal_and_vertical\"))\n",
    "        model.add(layers.RandomRotation(0.2))\n",
    "        \n",
    "        # --- Convolutional Block 1 ---\n",
    "        filters1 = hp.Int('filters1', min_value=32, max_value=64, step=8)\n",
    "        model.add(layers.Conv2D(filters1, kernel_size=3, strides=1, padding=\"same\", activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(layers.Dropout(0.25))\n",
    "        \n",
    "        # --- Convolutional Block 2 ---\n",
    "        filters2 = hp.Int('filters2', min_value=64, max_value=128, step=16)\n",
    "        model.add(layers.Conv2D(filters2, kernel_size=3, strides=1, padding=\"same\", activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(layers.Dropout(0.25))\n",
    "        \n",
    "        # --- Convolutional Block 3 ---\n",
    "        filters3 = hp.Int('filters3', min_value=128, max_value=256, step=32)\n",
    "        model.add(layers.Conv2D(filters3, kernel_size=3, strides=1, padding=\"same\", activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(layers.Dropout(0.25))\n",
    "        \n",
    "        # Global pooling using GlobalAveragePooling2D\n",
    "        model.add(layers.GlobalAveragePooling2D())\n",
    "        \n",
    "        # Fully connected (dense) layers\n",
    "        dense_units = hp.Int('dense_units', min_value=32, max_value=128, step=16)\n",
    "        model.add(layers.Dense(dense_units, activation='relu'))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "        \n",
    "        # Output layer for binary classification\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        # Hyperparameter for learning rate\n",
    "        lr = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "        \n",
    "        # Compile model with custom focal loss\n",
    "        loss_fn = focal_loss(gamma=2.0, alpha=0.25)\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_fn,\n",
    "            metrics=[\n",
    "                keras.metrics.BinaryAccuracy(name='binary_accuracy'),\n",
    "                keras.metrics.AUC(name='auc'),\n",
    "                # You can add additional metrics if desired.\n",
    "            ]\n",
    "        )\n",
    "        return model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e68e967f-623f-4332-a38c-57359b8121c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, hp, model, *args, **kwargs):\n",
    "     batch_size = hp.Int('batch_size', min_value=16, max_value=64, step=8)\n",
    "     return model.fit(*args, batch_size=batch_size, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b0947fd-6eb0-4f20-8b0f-2f5d11ab5b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 5\n",
      "filters1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 64, 'step': 8, 'sampling': 'linear'}\n",
      "filters2 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 128, 'step': 16, 'sampling': 'linear'}\n",
      "filters3 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 128, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n",
      "dense_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 128, 'step': 16, 'sampling': 'linear'}\n",
      "lr (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
     ]
    }
   ],
   "source": [
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    hypermodel=MyImprovedHyperModel(),\n",
    "    objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"),\n",
    "    max_trials=10,   # Increase for a broader search if needed.\n",
    "    overwrite=True,\n",
    "    seed=123\n",
    ")\n",
    "tuner.search_space_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a28ca26-7222-4c9c-a7d3-5217442d7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks: EarlyStopping and ReduceLROnPlateau\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "440bcae0-2441-4ee7-b688-446cc3951550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 13s]\n",
      "val_auc: 0.6983333230018616\n",
      "\n",
      "Best val_auc So Far: 0.7516666650772095\n",
      "Total elapsed time: 00h 02m 44s\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "tuner.search(\n",
    "    input_train_scaled,\n",
    "    label_train_scaled,\n",
    "    epochs=100,\n",
    "    validation_split=0.1,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weight,  # Using your computed class weights\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff5a6396-3b45-4ae6-91ad-62b07260777e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/u/home/rmandava/.local/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 34 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ random_flip (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomFlip</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ random_rotation                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomRotation</span>)                │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,568</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">193,760</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">21,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ random_flip (\u001b[38;5;33mRandomFlip\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m19\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ random_rotation                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m19\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mRandomRotation\u001b[0m)                │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m48\u001b[0m)     │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m48\u001b[0m)     │           \u001b[38;5;34m192\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m48\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m48\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │        \u001b[38;5;34m41,568\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m96\u001b[0m)     │           \u001b[38;5;34m384\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m96\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m96\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m224\u001b[0m)      │       \u001b[38;5;34m193,760\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m224\u001b[0m)      │           \u001b[38;5;34m896\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m224\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m224\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │        \u001b[38;5;34m21,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m97\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,753</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m266,753\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,017</span> (1.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m266,017\u001b[0m (1.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">736</span> (2.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m736\u001b[0m (2.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - auc: 0.9189 - binary_accuracy: 0.8417 - loss: 0.0085 - val_auc: 0.7333 - val_binary_accuracy: 0.8036 - val_loss: 0.0691 - learning_rate: 1.3955e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - auc: 0.9226 - binary_accuracy: 0.8699 - loss: 0.0080 - val_auc: 0.7467 - val_binary_accuracy: 0.7500 - val_loss: 0.0744 - learning_rate: 1.3955e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - auc: 0.8943 - binary_accuracy: 0.8524 - loss: 0.0096 - val_auc: 0.7467 - val_binary_accuracy: 0.8214 - val_loss: 0.0708 - learning_rate: 1.3955e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - auc: 0.9286 - binary_accuracy: 0.8580 - loss: 0.0077 - val_auc: 0.7400 - val_binary_accuracy: 0.7857 - val_loss: 0.0793 - learning_rate: 1.3955e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - auc: 0.9456 - binary_accuracy: 0.8707 - loss: 0.0070 - val_auc: 0.7350 - val_binary_accuracy: 0.8393 - val_loss: 0.0706 - learning_rate: 1.3955e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m15/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - auc: 0.9285 - binary_accuracy: 0.8715 - loss: 0.0082\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 6.97755313012749e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - auc: 0.9295 - binary_accuracy: 0.8717 - loss: 0.0080 - val_auc: 0.7267 - val_binary_accuracy: 0.8036 - val_loss: 0.0764 - learning_rate: 1.3955e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - auc: 0.9406 - binary_accuracy: 0.8709 - loss: 0.0074 - val_auc: 0.7183 - val_binary_accuracy: 0.8393 - val_loss: 0.0720 - learning_rate: 6.9776e-05\n",
      "Epoch 8/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - auc: 0.9657 - binary_accuracy: 0.9040 - loss: 0.0058 - val_auc: 0.7200 - val_binary_accuracy: 0.8571 - val_loss: 0.0705 - learning_rate: 6.9776e-05\n",
      "Epoch 9/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - auc: 0.9536 - binary_accuracy: 0.8696 - loss: 0.0066 - val_auc: 0.7133 - val_binary_accuracy: 0.8750 - val_loss: 0.0673 - learning_rate: 6.9776e-05\n",
      "Epoch 10/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - auc: 0.9331 - binary_accuracy: 0.8683 - loss: 0.0079 - val_auc: 0.6950 - val_binary_accuracy: 0.8393 - val_loss: 0.0680 - learning_rate: 6.9776e-05\n",
      "Epoch 11/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - auc: 0.9677 - binary_accuracy: 0.9000 - loss: 0.0057 - val_auc: 0.7117 - val_binary_accuracy: 0.8571 - val_loss: 0.0755 - learning_rate: 6.9776e-05\n",
      "Epoch 12/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - auc: 0.9460 - binary_accuracy: 0.9030 - loss: 0.0068 - val_auc: 0.7183 - val_binary_accuracy: 0.8571 - val_loss: 0.0779 - learning_rate: 6.9776e-05\n",
      "Epoch 13/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - auc: 0.9502 - binary_accuracy: 0.8878 - loss: 0.0064 - val_auc: 0.7200 - val_binary_accuracy: 0.8750 - val_loss: 0.0759 - learning_rate: 6.9776e-05\n",
      "Epoch 14/100\n",
      "\u001b[1m15/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - auc: 0.9636 - binary_accuracy: 0.8948 - loss: 0.0057\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 3.488776565063745e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - auc: 0.9632 - binary_accuracy: 0.8948 - loss: 0.0056 - val_auc: 0.7217 - val_binary_accuracy: 0.8750 - val_loss: 0.0773 - learning_rate: 6.9776e-05\n",
      "Epoch 15/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - auc: 0.9620 - binary_accuracy: 0.8883 - loss: 0.0057 - val_auc: 0.7133 - val_binary_accuracy: 0.8750 - val_loss: 0.0785 - learning_rate: 3.4888e-05\n",
      "Epoch 16/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - auc: 0.9712 - binary_accuracy: 0.9025 - loss: 0.0054 - val_auc: 0.7150 - val_binary_accuracy: 0.8750 - val_loss: 0.0775 - learning_rate: 3.4888e-05\n",
      "Epoch 17/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - auc: 0.9645 - binary_accuracy: 0.8871 - loss: 0.0059 - val_auc: 0.7217 - val_binary_accuracy: 0.8750 - val_loss: 0.0785 - learning_rate: 3.4888e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - auc: 0.9665 - binary_accuracy: 0.9157 - loss: 0.0057 - val_auc: 0.7183 - val_binary_accuracy: 0.8750 - val_loss: 0.0825 - learning_rate: 3.4888e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m15/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - auc: 0.9765 - binary_accuracy: 0.9158 - loss: 0.0051\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.7443882825318724e-05.\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - auc: 0.9761 - binary_accuracy: 0.9152 - loss: 0.0051 - val_auc: 0.7200 - val_binary_accuracy: 0.8393 - val_loss: 0.0820 - learning_rate: 3.4888e-05\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - auc: 0.8517 - binary_accuracy: 0.8857 - loss: 0.0465\n",
      "Evaluation results: [0.046546999365091324, 0.8857142925262451, 0.8516666889190674]\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predictions shape: (140, 1)\n",
      "Confusion Matrix:\n",
      "TN: 111 FP: 9 FN: 7 TP: 13\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the best model.\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "print(best_model.summary())\n",
    "\n",
    "# Optionally retrain the best model further.\n",
    "history = best_model.fit(\n",
    "    input_train_scaled,\n",
    "    label_train_scaled,\n",
    "    epochs=100,\n",
    "    validation_split=0.1,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weight,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Evaluate the improved model on the test set.\n",
    "# ---------------------------\n",
    "results = best_model.evaluate(input_test_scaled, label_test_scaled, batch_size=label_test_scaled.shape[0])\n",
    "print(\"Evaluation results:\", results)\n",
    "\n",
    "# Generate predictions.\n",
    "predictions = best_model.predict(input_test_scaled)\n",
    "print(\"Predictions shape:\", predictions.shape)\n",
    "\n",
    "# Optionally adjust the classification threshold (here set to 0.4).\n",
    "optimal_threshold = 0.5\n",
    "predicted_classes = (predictions > optimal_threshold).astype(int)\n",
    "\n",
    "# Compute and print the confusion matrix.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "tn, fp, fn, tp = confusion_matrix(label_test_scaled, predicted_classes).ravel()\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"TN:\", tn, \"FP:\", fp, \"FN:\", fn, \"TP:\", tp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40b46d58-ad34-4817-a519-c9152372b7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold=0.00 -> Precision=0.14, Recall=1.00\n",
      "Threshold=0.05 -> Precision=0.16, Recall=1.00\n",
      "Threshold=0.10 -> Precision=0.19, Recall=1.00\n",
      "Threshold=0.15 -> Precision=0.23, Recall=0.95\n",
      "Threshold=0.20 -> Precision=0.25, Recall=0.95\n",
      "Threshold=0.25 -> Precision=0.27, Recall=0.95\n",
      "Threshold=0.30 -> Precision=0.27, Recall=0.85\n",
      "Threshold=0.35 -> Precision=0.28, Recall=0.75\n",
      "Threshold=0.40 -> Precision=0.32, Recall=0.65\n",
      "Threshold=0.45 -> Precision=0.42, Recall=0.65\n",
      "Threshold=0.50 -> Precision=0.59, Recall=0.65\n",
      "Threshold=0.55 -> Precision=0.61, Recall=0.55\n",
      "Threshold=0.60 -> Precision=0.82, Recall=0.45\n",
      "Threshold=0.65 -> Precision=0.71, Recall=0.25\n",
      "Threshold=0.70 -> Precision=0.67, Recall=0.10\n",
      "Threshold=0.75 -> Precision=0.50, Recall=0.05\n",
      "Threshold=0.80 -> Precision=0.00, Recall=0.00\n",
      "Threshold=0.85 -> Precision=0.00, Recall=0.00\n",
      "Threshold=0.90 -> Precision=0.00, Recall=0.00\n",
      "Threshold=0.95 -> Precision=0.00, Recall=0.00\n",
      "Threshold=1.00 -> Precision=0.00, Recall=0.00\n"
     ]
    }
   ],
   "source": [
    "thresholds = np.arange(0, 1.01, 0.05)\n",
    "for t in thresholds:\n",
    "    preds_t = (predictions >= t).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(label_test_scaled, preds_t).ravel()\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    print(f\"Threshold={t:.2f} -> Precision={precision:.2f}, Recall={recall:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2149073e-96a2-4825-93b8-b1c979d91907",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe650ca-beff-44f0-8c90-69af9bc30138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
