{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2203c2-df40-428c-ae4c-71b20ba050c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bf88bb8-756d-47cb-ad74-88b3b302a462",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.022322,
     "end_time": "2025-03-05T18:49:06.250982",
     "exception": false,
     "start_time": "2025-03-05T18:49:06.228660",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "Parameters",
     "parameters",
     "  Parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "var_list = [\"default1\", \"default2\"]\n",
    "plevel_list = [False, 300]\n",
    "aew_subset = \"default_subset\"\n",
    "model_save_name = \"default_modelbase1.keras\"\n",
    "tuner_project_name = \"default_tuner_run1\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6798e3f",
   "metadata": {
    "papermill": {
     "duration": 0.012789,
     "end_time": "2025-03-05T18:49:06.269446",
     "exception": false,
     "start_time": "2025-03-05T18:49:06.256657",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# Parameters\n",
    "var_list = [\"cape\", \"crr\", \"d\", \"ie\", \"ishf\", \"lsrr\", \"pv\", \"q\",\"r\", \"sp\", \"sstk\", \"tcw\", \"tcwv\", \"t\", \"ttr\", \"u\",\"v\", \"vo\",\"w\"] #ERA5 variables\n",
    "\n",
    "\n",
    "\n",
    "plevel_list = [False, False,300, False, False, False, 300, 300, 300,False, False, False, False, 300,  False, 300,300  ,  300, 300] #pressure levels of variables\n",
    "\n",
    "aew_subset = \"12hr_before\"\n",
    "model_save_name = \"best_model_var(300)1234.keras\"\n",
    "tuner_project_name = \"tuner_run(300)1234\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe4b521-b034-49f0-a1ac-d961fe639066",
   "metadata": {
    "papermill": {
     "duration": 0.005028,
     "end_time": "2025-03-05T18:49:06.279538",
     "exception": false,
     "start_time": "2025-03-05T18:49:06.274510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d1aa354-4561-4486-9f61-73f2c1c81a31",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 13.973479,
     "end_time": "2025-03-05T18:49:20.258119",
     "exception": false,
     "start_time": "2025-03-05T18:49:06.284640",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 00:33:07.836928: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-29 00:33:07.956182: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-29 00:33:08.369805: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-29 00:33:08.369861: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-29 00:33:08.374659: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-29 00:33:08.445689: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-29 00:33:08.447081: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-29 00:33:14.854874: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import sklearn.model_selection\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras import layers\n",
    "\n",
    "import keras_tuner\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "keras.utils.set_random_seed(812)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32c4826e-dbc1-45c0-aca0-bc38d0254f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    \"\"\"Focal Loss for binary classification.\"\"\"\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        # Clip to prevent NaNs \n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        bce = tf.keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        modulating_factor = tf.pow(1.0 - p_t, gamma)\n",
    "        alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        return alpha_factor * modulating_factor * bce\n",
    "    return loss_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c74c2082-8917-4d0d-a2d7-b39f8548bcbb",
   "metadata": {
    "papermill": {
     "duration": 0.014754,
     "end_time": "2025-03-05T18:49:20.282885",
     "exception": false,
     "start_time": "2025-03-05T18:49:20.268131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#functions to calculate f1 score as loss function\n",
    "\n",
    "def f1_loss_sigmoid(y_true, y_pred):\n",
    "    \"\"\"\n",
    "\n",
    "    F1 metric for sigmoid output and integer encoded labels.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # compute tp, fp, and fn\n",
    "\n",
    "    tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n",
    "\n",
    "    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\n",
    "\n",
    "    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\n",
    "\n",
    "\n",
    "    # precision (tp / (tp + fp))\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "\n",
    "   # recall (tp / (tp + fn))\n",
    "\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "\n",
    "# harmonic mean of precision and recall\n",
    "\n",
    "    f1 = (2 * p * r) / (p + r + K.epsilon())\n",
    "\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "\n",
    "    return 1 - K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58fe83a8-b5b0-4a10-81f6-a5b9277e5a3a",
   "metadata": {
    "papermill": {
     "duration": 0.012029,
     "end_time": "2025-03-05T18:49:20.300625",
     "exception": false,
     "start_time": "2025-03-05T18:49:20.288596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f1_loss_onehot(y_true, y_pred):\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "   F1 metric for two-class output and one-hot encoded labels.\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "   # compute tp, fp, and fn\n",
    "\n",
    "   tp = K.sum(K.cast(y_true[:, 1] * y_pred[:, 1], 'float'), axis=0)\n",
    "\n",
    "   fp = K.sum(K.cast((1 - y_true[:, 1]) * y_pred[:, 1], 'float'), axis=0)\n",
    "\n",
    "   fn = K.sum(K.cast(y_true[:, 0] * (1 - y_pred[:, 0]), 'float'), axis=0)\n",
    "\n",
    "\n",
    "   # precision (tp / (tp + fp))\n",
    "\n",
    "   p = tp / (tp + fp + K.epsilon())\n",
    "\n",
    "   # recall (tp / (tp + fn))\n",
    "\n",
    "   r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "\n",
    "   # harmonic mean of precision and recall\n",
    "\n",
    "   f1 = (2 * p * r) / (p + r + K.epsilon())\n",
    "\n",
    "   f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "\n",
    "   return 1 - K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82e6df90-377d-4c27-ac1d-156b3bc34623",
   "metadata": {
    "papermill": {
     "duration": 0.012229,
     "end_time": "2025-03-05T18:49:20.319927",
     "exception": false,
     "start_time": "2025-03-05T18:49:20.307698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def add_dim(ds):\n",
    "    # Extract the source file name from the dataset's encoding.\n",
    "    fname = ds.encoding.get('source', '')\n",
    "    # Use a regex to capture the central latitude and longitude from the filename.\n",
    "    m = re.search(r'_(\\-?\\d+\\.\\d+)_(-?\\d+\\.\\d+)\\.nc$', fname)\n",
    "    if m:\n",
    "        lat_center = float(m.group(1))\n",
    "        lon_center = float(m.group(2))\n",
    "        # Assign the central coordinates and the file name as new coordinates.\n",
    "        ds = ds.assign_coords(lat_center=lat_center, lon_center=lon_center, file_name=fname)\n",
    "    else:\n",
    "        print(\"File name does not match expected pattern:\", fname)\n",
    "    \n",
    "    # Expand dims to add the 'sample' dimension and drop unnecessary variables.\n",
    "    return ds.assign_coords({\"sample\": 1}).expand_dims(dim={\"sample\": 1}).drop_vars(\"utc_date\").drop_vars(\"latitude\").drop_vars(\"longitude\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec3de133-3ed1-4df3-9444-a58e88716284",
   "metadata": {
    "papermill": {
     "duration": 0.017203,
     "end_time": "2025-03-05T18:49:20.344038",
     "exception": false,
     "start_time": "2025-03-05T18:49:20.326835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "def open_files_zarr(list_of_vars, aew_subset=\"12hr_before\",\n",
    "                    directory=\"/glade/derecho/scratch/rmandava/AEW_time_location_files/\",\n",
    "                    plevel_list=None, zarr_store_path=\"zarr_data\"):\n",
    "    \"\"\"\n",
    "    Opens ERA5 NetCDF files for the given variables. For each variable (or pressure-level variant),\n",
    "    it checks if a corresponding Zarr store exists in 'zarr_store_path'. If so, it loads the dataset\n",
    "    from the Zarr store; if not, it opens the NetCDF files, preprocesses them, saves them to Zarr,\n",
    "    and then returns the dataset.\n",
    "    \"\"\"\n",
    "    # Create the zarr_store_path directory if it doesn't exist.\n",
    "    if not os.path.exists(zarr_store_path):\n",
    "        os.makedirs(zarr_store_path)\n",
    "    \n",
    "    datas = {}\n",
    "    for num, var in enumerate(list_of_vars):\n",
    "        # Determine the key and filename based on whether a pressure level is specified.\n",
    "        if plevel_list:\n",
    "            if plevel_list[num]:\n",
    "                key = f\"{var}_{int(plevel_list[num])}\"\n",
    "                file_pattern = f'{directory}/{var}/aew_{aew_subset}_{int(plevel_list[num])}_*.nc'\n",
    "            else:\n",
    "                key = var\n",
    "                file_pattern = f'{directory}/{var}/aew_{aew_subset}_*.nc'\n",
    "        else:\n",
    "            key = var\n",
    "            file_pattern = f'{directory}/{var}/aew_{aew_subset}_*.nc'\n",
    "        \n",
    "        # Define the zarr path for this variable.\n",
    "        zarr_path = os.path.join(zarr_store_path, f\"{key}.zarr\")\n",
    "        \n",
    "        # If the Zarr dataset exists, load from it; otherwise, create it.\n",
    "        if os.path.exists(zarr_path):\n",
    "            print(f\"Loading {key} from Zarr store.\")\n",
    "            ds = xr.open_zarr(zarr_path)\n",
    "        else:\n",
    "            print(f\"Creating Zarr store for {key} from NetCDF files.\")\n",
    "            ds = xr.open_mfdataset(\n",
    "                file_pattern,\n",
    "                preprocess=add_dim,\n",
    "                concat_dim=\"sample\",\n",
    "                combine=\"nested\",\n",
    "            )\n",
    "            ds.to_zarr(zarr_path, mode=\"w\")\n",
    "        datas[key] = ds\n",
    "    \n",
    "    return datas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c80e6-2437-4626-be0b-4d82bebb4f73",
   "metadata": {
    "papermill": {
     "duration": 0.015473,
     "end_time": "2025-03-05T18:49:20.366598",
     "exception": false,
     "start_time": "2025-03-05T18:49:20.351125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad1f231e-abc1-49e7-9bab-09ab1f3f4ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_load_concat(data_dictionary):\n",
    "    # Instead of eagerly converting to NumPy arrays, keep the datasets as xarray objects.\n",
    "    transposed = {}\n",
    "    for key, ds in data_dictionary.items():\n",
    "        var_name = key.split('_')[0].upper()\n",
    "        # Do lazy transpose and add a 'features' dimension\n",
    "        transposed[key] = ds[var_name].expand_dims('features').transpose('sample', 'latitude', 'longitude', 'features')\n",
    "    # Concatenate along the new 'features' dimension (if multiple variables exist)\n",
    "    if len(transposed) > 1:\n",
    "        data = xr.concat(list(transposed.values()), dim='features',coords='minimal',compat='override')\n",
    "    else:\n",
    "        data = list(transposed.values())[0]\n",
    "    # Use the coordinates (lat_center, lon_center) from one of the datasets.\n",
    "    # They remain lazy and are not computed until needed.\n",
    "    first_key = next(iter(data_dictionary))\n",
    "    lat_center = data_dictionary[first_key]['lat_center']\n",
    "    lon_center = data_dictionary[first_key]['lon_center']\n",
    "    label = data_dictionary[first_key]['label']\n",
    "    return data, label, lat_center, lon_center\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bec2717c-8176-447f-82ea-5b15e038ace1",
   "metadata": {
    "papermill": {
     "duration": 0.010767,
     "end_time": "2025-03-05T18:49:20.384143",
     "exception": false,
     "start_time": "2025-03-05T18:49:20.373376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def omit_nans(data, label, lat, lon):\n",
    "    # If data is an xarray DataArray, convert it to a NumPy array\n",
    "    if hasattr(data, 'values'):\n",
    "        data = data.values\n",
    "    maskarray = np.full(data.shape[0], True)\n",
    "    # Find indices where NaNs occur\n",
    "    masker = np.unique(np.argwhere(np.isnan(data))[:, 0])\n",
    "    maskarray[masker] = False\n",
    "\n",
    "    traindata = data[maskarray, ...]\n",
    "    trainlabel = label[maskarray]\n",
    "    lat_filtered = lat[maskarray]\n",
    "    lon_filtered = lon[maskarray]\n",
    "    return traindata, trainlabel, lat_filtered, lon_filtered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d02dbe1a-f1ba-4f9b-8fe0-d9126814ae62",
   "metadata": {
    "papermill": {
     "duration": 0.010866,
     "end_time": "2025-03-05T18:49:20.400498",
     "exception": false,
     "start_time": "2025-03-05T18:49:20.389632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def zscore(data):\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  Rescaling the data using zscore (mean/std).\n",
    "\n",
    "  Each variable gets scaled independently from others.\n",
    "\n",
    "  Note that we will need to remove test data for formal training.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  for i in range(0, data_.shape[-1]):\n",
    "\n",
    "      data_[:, :, :, i] = (\n",
    "\n",
    "               data_[:, :, :, i] - np.nanmean(\n",
    "\n",
    "                     data_[:, :, :, i])) / np.nanstd(data_[:, :, :, i])\n",
    "\n",
    "  return data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "332bc130-a4e7-465c-a135-687f543db181",
   "metadata": {
    "papermill": {
     "duration": 0.011066,
     "end_time": "2025-03-05T18:49:20.416849",
     "exception": false,
     "start_time": "2025-03-05T18:49:20.405783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def minmax(data):\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "   Rescaling the data using min-max.\n",
    "\n",
    "   Each variable gets scaled independently from others.\n",
    "\n",
    "   Note that we will need to remove test data for formal training.\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "   for i in range(0, data_.shape[-1]):\n",
    "\n",
    "          data_[:, :, :, i] = (\n",
    "\n",
    "              data_[:, :, :, i] - np.nanmin(data_[:, :, :, i])\n",
    "\n",
    "          ) / (np.nanmax(data_[:, :, :, i]) - np.nanmin(data_[:, :, :, i]))\n",
    "   return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f652c778-f81b-4c46-bf87-d60487903b73",
   "metadata": {
    "papermill": {
     "duration": 0.010904,
     "end_time": "2025-03-05T18:49:20.433062",
     "exception": false,
     "start_time": "2025-03-05T18:49:20.422158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_split(data, label, split=0.3, seed=0):\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "   Help spliting data randomly for training and testing.\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "   np.random.seed(0)\n",
    "\n",
    "   da_indx = np.random.permutation(data.shape[0])\n",
    "\n",
    "   data = data[da_indx.astype(int)]\n",
    "\n",
    "   label = label[da_indx.astype(int)]\n",
    "\n",
    "   init_range = int(data.shape[0] * (1 - 0.3))\n",
    "\n",
    "   return data[:init_range], label[:init_range], data[init_range:], label[init_range:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9f970b2-2444-4dba-8e66-a850482d5ea4",
   "metadata": {
    "papermill": {
     "duration": 0.010837,
     "end_time": "2025-03-05T18:49:20.449475",
     "exception": false,
     "start_time": "2025-03-05T18:49:20.438638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pick_loss(loss_string):\n",
    "    \n",
    "\n",
    "    lossdict = {\n",
    "        \"relu\": tf.nn.relu,\n",
    "        \"tanh\": tf.nn.tanh,\n",
    "        \"selu\": tf.nn.selu,\n",
    "        \"sigmoid\": tf.nn.sigmoid,\n",
    "        \"relu6\": tf.nn.relu6,\n",
    "        \"silu\": tf.nn.silu,\n",
    "        \"gelu\": tf.nn.gelu,\n",
    "        \"lrelu\": tf.nn.leaky_relu,\n",
    "    }\n",
    "\n",
    "    return lossdict[loss_string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3e0c99e-885a-4f64-b057-0056b0ef246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency_map(model, input_sample):\n",
    "    \"\"\"\n",
    "    Compute a saliency map for a given input sample using a gradient-based approach.\n",
    "    \n",
    "    Args:\n",
    "        model (tf.keras.Model): The trained Keras model.\n",
    "        input_sample (numpy array): A single input sample of shape (1, height, width, channels).\n",
    "    \n",
    "    Returns:\n",
    "        saliency (numpy array): The saliency map of shape (height, width).\n",
    "    \"\"\"\n",
    "    # Ensure the model is in inference mode\n",
    "    model.trainable = False\n",
    "    input_tensor = tf.convert_to_tensor(input_sample)\n",
    "    \n",
    "    # Use GradientTape to record operations for automatic differentiation\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Watch the input tensor\n",
    "        tape.watch(input_tensor)\n",
    "        # Get the model's prediction\n",
    "        prediction = model(input_tensor)\n",
    "    \n",
    "    # Compute gradients of the prediction with respect to the input\n",
    "    grads = tape.gradient(prediction, input_tensor)\n",
    "    \n",
    "    # If there are multiple channels, take the maximum absolute gradient across channels\n",
    "    saliency = np.max(np.abs(grads.numpy()), axis=-1)[0]\n",
    "    return saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9776cdc0-38bc-476f-b93d-3753a2eb6803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency_per_channel(model, input_sample):\n",
    "    \"\"\"\n",
    "    Computes the saliency map for each channel of a given input sample.\n",
    "    \n",
    "    Args:\n",
    "        model (tf.keras.Model): The trained Keras model.\n",
    "        input_sample (numpy array): A single input sample with shape (1, H, W, C).\n",
    "        \n",
    "    Returns:\n",
    "        saliency_maps (numpy array): Absolute gradients with shape (H, W, C) for each channel.\n",
    "        channel_importance (numpy array): Mean saliency per channel (shape: (C,)).\n",
    "    \"\"\"\n",
    "    # Set the model to inference mode\n",
    "    model.trainable = False\n",
    "    input_tensor = tf.convert_to_tensor(input_sample)\n",
    "    \n",
    "    # Compute gradients with respect to the input sample using GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(input_tensor)\n",
    "        prediction = model(input_tensor)\n",
    "    \n",
    "    # Calculate gradients: shape (1, H, W, C)\n",
    "    grads = tape.gradient(prediction, input_tensor)\n",
    "    \n",
    "    # Remove the batch dimension: shape becomes (H, W, C)\n",
    "    grads = grads.numpy()[0]\n",
    "    \n",
    "    # Take absolute value to measure importance (magnitude of sensitivity)\n",
    "    saliency_maps = np.abs(grads)\n",
    "    \n",
    "    # Aggregate saliency per channel (e.g., using the mean over spatial dimensions)\n",
    "    channel_importance = np.mean(saliency_maps, axis=(0, 1))\n",
    "    \n",
    "    return saliency_maps, channel_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81aa5f1b-5513-4496-9fae-a6cde06e1aaa",
   "metadata": {
    "papermill": {
     "duration": 0.00964,
     "end_time": "2025-03-05T18:49:20.464813",
     "exception": false,
     "start_time": "2025-03-05T18:49:20.455173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_features = len(var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3490622e-a626-42aa-9cfe-020ee54d21e0",
   "metadata": {
    "papermill": {
     "duration": 758.818091,
     "end_time": "2025-03-05T19:01:59.288567",
     "exception": false,
     "start_time": "2025-03-05T18:49:20.470476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cape from Zarr store.\n",
      "Loading crr from Zarr store.\n",
      "Loading d_300 from Zarr store.\n",
      "Loading ie from Zarr store.\n",
      "Loading ishf from Zarr store.\n",
      "Loading lsrr from Zarr store.\n",
      "Loading pv_300 from Zarr store.\n",
      "Loading q_300 from Zarr store.\n",
      "Loading r_300 from Zarr store.\n",
      "Loading sp from Zarr store.\n",
      "Loading sstk from Zarr store.\n",
      "Loading tcw from Zarr store.\n",
      "Loading tcwv from Zarr store.\n",
      "Loading t_300 from Zarr store.\n",
      "Loading ttr from Zarr store.\n",
      "Loading u_300 from Zarr store.\n",
      "Loading v_300 from Zarr store.\n",
      "Loading vo_300 from Zarr store.\n",
      "Loading w_300 from Zarr store.\n"
     ]
    }
   ],
   "source": [
    "data = open_files_zarr(\n",
    "    list_of_vars=var_list,\n",
    "    aew_subset=aew_subset,\n",
    "    directory=\"/glade/derecho/scratch/rmandava/AEW_time_location_files/\",\n",
    "    plevel_list=plevel_list,\n",
    "    zarr_store_path=\"/glade/derecho/scratch/rmandava/AEW_time_location_files/Project1/zarr\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ddac3e-071b-4796-9ca6-9cb01deeb598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f240d4af-46d1-4cef-a5ab-2e2527a1639f",
   "metadata": {
    "papermill": {
     "duration": 185.616237,
     "end_time": "2025-03-05T19:05:04.925249",
     "exception": false,
     "start_time": "2025-03-05T19:01:59.309012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2750, 32, 32, 19)\n"
     ]
    }
   ],
   "source": [
    "# transpose the data and concat variables\n",
    "\n",
    "#data_, labels_ = transpose_load_concat(data)\n",
    "data_, labels_, sample_lat, sample_lon = transpose_load_concat(data)\n",
    "\n",
    "print(np.shape(data_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0e4f7b8-f9e0-4131-ae35-97e47bc557d0",
   "metadata": {
    "papermill": {
     "duration": 0.372532,
     "end_time": "2025-03-05T19:05:05.305951",
     "exception": false,
     "start_time": "2025-03-05T19:05:04.933419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check / remove nans\n",
    "\n",
    "data_, labels_, sample_lat, sample_lon = omit_nans(data_, labels_, sample_lat, sample_lon)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0492935e-1f35-446b-ab16-aba0e667857b",
   "metadata": {
    "papermill": {
     "duration": 0.044227,
     "end_time": "2025-03-05T19:05:05.358050",
     "exception": false,
     "start_time": "2025-03-05T19:05:05.313823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(560, 32, 32, 19) (140, 32, 32, 19) (560,) (140,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/u/home/rmandava/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1642: PerformanceWarning: Slicing with an out-of-order index is generating 106 times more chunks\n",
      "  return self.array[key]\n",
      "/glade/u/home/rmandava/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1642: PerformanceWarning: Slicing with an out-of-order index is generating 27 times more chunks\n",
      "  return self.array[key]\n"
     ]
    }
   ],
   "source": [
    "#split train and test set\n",
    "X_train, X_test, y_train, y_test, lat_train, lat_test, lon_train, lon_test = sklearn.model_selection.train_test_split(\n",
    "    data_, labels_, sample_lat, sample_lon, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "print (np.shape(X_train), np.shape(X_test), np.shape(y_train), np.shape(y_test))\n",
    "\n",
    "\n",
    "y_train = np.expand_dims(y_train, axis=1)\n",
    "\n",
    "y_test = np.expand_dims(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "164ee316-7c83-474b-bdb9-7c12ca165e1f",
   "metadata": {
    "papermill": {
     "duration": 0.312834,
     "end_time": "2025-03-05T19:05:05.678612",
     "exception": false,
     "start_time": "2025-03-05T19:05:05.365778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [21]: Scaling code (fixed to prevent data leakage)\n",
    "\n",
    "# Create the scaler object\n",
    "scaler_input = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "# Reshape training data to 2D (samples, features)\n",
    "X_train_tmp = np.reshape(X_train, (-1, len(var_list)))\n",
    "\n",
    "# Fit the scaler ONLY on the training data\n",
    "scaler_input.fit(X_train_tmp)  # <-- Key change: Learn mean/std from training data\n",
    "\n",
    "# Transform BOTH training and test data using the SAME scaler\n",
    "input_train_scaled = scaler_input.transform(X_train_tmp)          # Train: transform only\n",
    "input_test_scaled = scaler_input.transform(                       # Test: transform only\n",
    "    np.reshape(X_test, (-1, len(var_list)))\n",
    ")\n",
    "\n",
    "# Reshape back to original dimensions (samples, height, width, features)\n",
    "input_train_scaled = np.reshape(input_train_scaled, X_train.shape)\n",
    "input_test_scaled = np.reshape(input_test_scaled, X_test.shape)\n",
    "\n",
    "# Labels remain unchanged\n",
    "label_train_scaled = y_train\n",
    "label_test_scaled = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bed418e-264b-45e6-92ee-b72c2f7a04d7",
   "metadata": {
    "papermill": {
     "duration": 0.014239,
     "end_time": "2025-03-05T19:05:05.701116",
     "exception": false,
     "start_time": "2025-03-05T19:05:05.686877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(560, 32, 32, 19) (560, 1) (140, 32, 32, 19) (140, 1)\n"
     ]
    }
   ],
   "source": [
    "# print the shapes to double check them\n",
    "\n",
    "print(\n",
    "\n",
    "input_train_scaled.shape,\n",
    "\n",
    "label_train_scaled.shape,\n",
    "\n",
    "input_test_scaled.shape,\n",
    "\n",
    "label_test_scaled.shape\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10597eb7-f8fa-4523-a306-55d5a442d504",
   "metadata": {
    "papermill": {
     "duration": 0.017418,
     "end_time": "2025-03-05T19:05:05.726147",
     "exception": false,
     "start_time": "2025-03-05T19:05:05.708729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive samples in training data: 91 (16.25% of total)\n"
     ]
    }
   ],
   "source": [
    "#generate class weights due to class imbalance issues\n",
    "\n",
    "counts = np.bincount(y_train[:, 0].astype(int))\n",
    "\n",
    "\n",
    "print(\n",
    "\n",
    "\"Number of positive samples in training data: {} ({:.2f}% of total)\".format(\n",
    "\n",
    "counts[1], 100 * float(counts[1]) / len(y_train))\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "557d98a2-07ba-4fa7-8ba2-9ce8baa53b63",
   "metadata": {
    "papermill": {
     "duration": 0.01967,
     "end_time": "2025-03-05T19:05:05.753659",
     "exception": false,
     "start_time": "2025-03-05T19:05:05.733989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.1625, 1: 0.8375}\n"
     ]
    }
   ],
   "source": [
    "# old weights\n",
    "\n",
    "# weight_for_0 = 1.0 / counts[0]\n",
    "\n",
    "# weight_for_1 = 1.0 / counts[1]\n",
    "\n",
    "\n",
    "#new weights\n",
    "\n",
    "weight_for_0 = float(counts[1]) / len(y_train)\n",
    "\n",
    "weight_for_1 = 1 - (float(counts[1]) / len(y_train))\n",
    "\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "700f978d-cc6f-484a-864f-dcb9e703eed4",
   "metadata": {
    "papermill": {
     "duration": 1.029155,
     "end_time": "2025-03-05T19:05:06.790734",
     "exception": false,
     "start_time": "2025-03-05T19:05:05.761579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 00:35:32.035751: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "METRICS = [\n",
    "\n",
    "keras.metrics.BinaryCrossentropy(name='cross entropy'),\n",
    "\n",
    "keras.metrics.MeanSquaredError(name='mean_squared_error'),\n",
    "\n",
    "keras.metrics.RootMeanSquaredError(name='root_mean_squared_error'),\n",
    "\n",
    "keras.metrics.TruePositives(name='tp'),\n",
    "\n",
    "keras.metrics.FalsePositives(name='fp'),\n",
    "\n",
    "keras.metrics.TrueNegatives(name='tn'),\n",
    "\n",
    "keras.metrics.FalseNegatives(name='fn'),\n",
    "\n",
    "keras.metrics.BinaryAccuracy(name='binary_accuracy'),\n",
    "\n",
    "keras.metrics.F1Score(threshold=0.5, name='f1_score'),\n",
    "\n",
    "keras.metrics.Precision(name='precision'),\n",
    "\n",
    "keras.metrics.Recall(name='recall'),\n",
    "\n",
    "keras.metrics.AUC(name='auc'),\n",
    "\n",
    "keras.metrics.AUC(name='prc', curve='PR'),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9900636-a8fb-4b46-9282-66643c95cdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bc9aab7-2558-4304-a6c7-5809d60e8437",
   "metadata": {
    "papermill": {
     "duration": 0.030448,
     "end_time": "2025-03-05T19:05:06.829775",
     "exception": false,
     "start_time": "2025-03-05T19:05:06.799327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyHyperModel(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = keras.Sequential()\n",
    "\n",
    "        model.add(keras.Input(shape=(32, 32, number_of_features)))\n",
    "\n",
    "        # Data augmentation\n",
    "        model.add(layers.RandomFlip(\"horizontal_and_vertical\"))\n",
    "        model.add(layers.RandomRotation(factor=(-0.5, 0.5)))\n",
    "\n",
    "        # CNN layers (same tuning as before)\n",
    "        featmaps1 = hp.Int('units_1', min_value=10, max_value=60)\n",
    "        featmaps2 = hp.Int('units_2', min_value=10, max_value=64)\n",
    "        featmaps3 = hp.Int('units_3', min_value=10, max_value=128)\n",
    "        featmaps4 = hp.Int('units_4', min_value=10, max_value=80)\n",
    "        learning_rate = hp.Float('lr', min_value=0.00001, max_value=0.01, sampling=\"linear\")\n",
    "        act_func = hp.Choice('activation', [\"relu\", \"tanh\", \"selu\", \"sigmoid\", \"relu6\", \"silu\", \"gelu\"])\n",
    "\n",
    "        model.add(layers.Conv2D(featmaps1, 3, strides=1, padding=\"same\", activation=pick_loss(act_func)))\n",
    "        model.add(layers.MaxPooling2D(2))\n",
    "        model.add(layers.Dropout(0.2))\n",
    "\n",
    "        model.add(layers.Conv2D(featmaps2, 3, strides=1, padding=\"same\", activation=pick_loss(act_func)))\n",
    "        model.add(layers.MaxPooling2D(2))\n",
    "        model.add(layers.Dropout(0.2))\n",
    "\n",
    "        model.add(layers.Conv2D(featmaps3, 3, strides=1, padding=\"same\", activation=pick_loss(act_func)))\n",
    "        model.add(layers.MaxPooling2D(2))\n",
    "        model.add(layers.Dropout(0.2))\n",
    "\n",
    "        model.add(layers.GlobalMaxPooling2D())\n",
    "        model.add(layers.Dense(featmaps4))\n",
    "        model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "        \n",
    "        gamma = hp.Float('gamma', min_value=1.0, max_value=3.0, step=0.5)\n",
    "        alpha = hp.Float('alpha', min_value=0.1, max_value=0.9, step=0.1)\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            loss=focal_loss(gamma=gamma, alpha=alpha),  # pass tuned gamma and alpha!\n",
    "            metrics=METRICS\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e68e967f-623f-4332-a38c-57359b8121c3",
   "metadata": {
    "papermill": {
     "duration": 0.018907,
     "end_time": "2025-03-05T19:05:06.856235",
     "exception": false,
     "start_time": "2025-03-05T19:05:06.837328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit(self, hp, model, *args, **kwargs):\n",
    "    batchsizenum = hp.Int('batch_size', min_value=10, max_value=60, step=5, sampling=\"linear\")\n",
    "\n",
    "    print({k: hp.get(k) if hp.is_active(k) else None for k in hp._hps})\n",
    "\n",
    "    return model.fit(\n",
    "        *args,\n",
    "        batch_size=batchsizenum,\n",
    "        # normally we might use early stopping, but not needed since\n",
    "        # callbacks saves checkpoints of the model during trials\n",
    "        # callbacks=keras.callbacks.EarlyStopping('val_loss', patience=5),\n",
    "        validation_split=0.1,\n",
    "        shuffle=True,\n",
    "        class_weight=class_weight,\n",
    "        **kwargs,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b0947fd-6eb0-4f20-8b0f-2f5d11ab5b3b",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.379052,
     "end_time": "2025-03-05T19:05:07.243192",
     "exception": false,
     "start_time": "2025-03-05T19:05:06.864140",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "Parameters",
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 8\n",
      "units_1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 10, 'max_value': 60, 'step': 1, 'sampling': 'linear'}\n",
      "units_2 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 10, 'max_value': 64, 'step': 1, 'sampling': 'linear'}\n",
      "units_3 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 10, 'max_value': 128, 'step': 1, 'sampling': 'linear'}\n",
      "units_4 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 10, 'max_value': 80, 'step': 1, 'sampling': 'linear'}\n",
      "lr (Float)\n",
      "{'default': 1e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 0.01, 'step': None, 'sampling': 'linear'}\n",
      "activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh', 'selu', 'sigmoid', 'relu6', 'silu', 'gelu'], 'ordered': False}\n",
      "gamma (Float)\n",
      "{'default': 1.0, 'conditions': [], 'min_value': 1.0, 'max_value': 3.0, 'step': 0.5, 'sampling': 'linear'}\n",
      "alpha (Float)\n",
      "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.9, 'step': 0.1, 'sampling': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# make the tuner object\n",
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    hypermodel=MyHyperModel(),\n",
    "    objective=keras_tuner.Objective(\"val_f1_score\", direction=\"max\"),\n",
    "    max_trials=150,\n",
    "    project_name=tuner_project_name,\n",
    "    alpha=0.0001,\n",
    "    beta=2.6,\n",
    "    seed=123,\n",
    "    tune_new_entries=True,\n",
    "    allow_new_entries=True,\n",
    "    max_retries_per_trial=1,\n",
    "    max_consecutive_failed_trials=3,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# summary\n",
    "tuner.search_space_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5a6396-3b45-4ae6-91ad-62b07260777e",
   "metadata": {
    "papermill": {
     "duration": 6055.188829,
     "end_time": "2025-03-05T20:46:02.439902",
     "exception": false,
     "start_time": "2025-03-05T19:05:07.251073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 23 Complete [00h 00m 19s]\n",
      "val_f1_score: 0.5\n",
      "\n",
      "Best val_f1_score So Far: 0.6666666865348816\n",
      "Total elapsed time: 00h 07m 56s\n",
      "\n",
      "Search: Running Trial #24\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "42                |49                |units_1\n",
      "20                |29                |units_2\n",
      "17                |48                |units_3\n",
      "49                |67                |units_4\n",
      "0.0082062         |0.0044452         |lr\n",
      "relu6             |relu              |activation\n",
      "2                 |3                 |gamma\n",
      "0.2               |0.9               |alpha\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.2601 - cross entropy: 1.1178 - mean_squared_error: 0.1819 - root_mean_squared_error: 0.4265 - tp: 7.0000 - fp: 32.0000 - tn: 437.0000 - fn: 84.0000 - binary_accuracy: 0.7929 - f1_score: 0.1077 - precision: 0.1795 - recall: 0.0769 - auc: 0.4690 - prc: 0.1682 - val_loss: 0.0781 - val_cross entropy: 0.5871 - val_mean_squared_error: 0.1973 - val_root_mean_squared_error: 0.4442 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5350 - val_prc: 0.1175\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0510 - cross entropy: 0.4780 - mean_squared_error: 0.1500 - root_mean_squared_error: 0.3873 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 419.0000 - fn: 85.0000 - binary_accuracy: 0.8313 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5591 - prc: 0.2330 - val_loss: 0.0581 - val_cross entropy: 0.5247 - val_mean_squared_error: 0.1676 - val_root_mean_squared_error: 0.4094 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6617 - val_prc: 0.2547\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0338 - cross entropy: 0.4293 - mean_squared_error: 0.1310 - root_mean_squared_error: 0.3620 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 419.0000 - fn: 85.0000 - binary_accuracy: 0.8313 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7513 - prc: 0.3795 - val_loss: 0.0413 - val_cross entropy: 0.4636 - val_mean_squared_error: 0.1401 - val_root_mean_squared_error: 0.3743 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7717 - val_prc: 0.3070\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0327 - cross entropy: 0.4145 - mean_squared_error: 0.1260 - root_mean_squared_error: 0.3550 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 419.0000 - fn: 85.0000 - binary_accuracy: 0.8313 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7855 - prc: 0.4522 - val_loss: 0.0290 - val_cross entropy: 0.4001 - val_mean_squared_error: 0.1144 - val_root_mean_squared_error: 0.3382 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7033 - val_prc: 0.2200\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0327 - cross entropy: 0.4031 - mean_squared_error: 0.1238 - root_mean_squared_error: 0.3518 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 419.0000 - fn: 85.0000 - binary_accuracy: 0.8313 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7916 - prc: 0.3692 - val_loss: 0.0343 - val_cross entropy: 0.4320 - val_mean_squared_error: 0.1270 - val_root_mean_squared_error: 0.3563 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7600 - val_prc: 0.2558\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0321 - cross entropy: 0.4106 - mean_squared_error: 0.1261 - root_mean_squared_error: 0.3551 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 419.0000 - fn: 85.0000 - binary_accuracy: 0.8313 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7961 - prc: 0.3544 - val_loss: 0.0329 - val_cross entropy: 0.4209 - val_mean_squared_error: 0.1228 - val_root_mean_squared_error: 0.3504 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7350 - val_prc: 0.2339\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0304 - cross entropy: 0.3846 - mean_squared_error: 0.1171 - root_mean_squared_error: 0.3422 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 419.0000 - fn: 85.0000 - binary_accuracy: 0.8313 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8248 - prc: 0.4509 - val_loss: 0.0321 - val_cross entropy: 0.4036 - val_mean_squared_error: 0.1165 - val_root_mean_squared_error: 0.3414 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7517 - val_prc: 0.2835\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0300 - cross entropy: 0.3850 - mean_squared_error: 0.1167 - root_mean_squared_error: 0.3416 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 419.0000 - fn: 85.0000 - binary_accuracy: 0.8313 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8396 - prc: 0.4562 - val_loss: 0.0358 - val_cross entropy: 0.4280 - val_mean_squared_error: 0.1265 - val_root_mean_squared_error: 0.3557 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6883 - val_prc: 0.2137\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0288 - cross entropy: 0.3707 - mean_squared_error: 0.1130 - root_mean_squared_error: 0.3361 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 419.0000 - fn: 85.0000 - binary_accuracy: 0.8313 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8628 - prc: 0.4460 - val_loss: 0.0259 - val_cross entropy: 0.3426 - val_mean_squared_error: 0.0966 - val_root_mean_squared_error: 0.3108 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7450 - val_prc: 0.2727\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0333 - cross entropy: 0.3688 - mean_squared_error: 0.1145 - root_mean_squared_error: 0.3384 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 419.0000 - fn: 85.0000 - binary_accuracy: 0.8313 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8294 - prc: 0.4643 - val_loss: 0.0273 - val_cross entropy: 0.3678 - val_mean_squared_error: 0.1037 - val_root_mean_squared_error: 0.3221 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7900 - val_prc: 0.3664\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0320 - cross entropy: 0.3894 - mean_squared_error: 0.1194 - root_mean_squared_error: 0.3455 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 419.0000 - fn: 85.0000 - binary_accuracy: 0.8313 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8128 - prc: 0.4479 - val_loss: 0.0482 - val_cross entropy: 0.4712 - val_mean_squared_error: 0.1457 - val_root_mean_squared_error: 0.3817 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6733 - val_prc: 0.2069\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0342 - cross entropy: 0.3954 - mean_squared_error: 0.1214 - root_mean_squared_error: 0.3485 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 419.0000 - fn: 85.0000 - binary_accuracy: 0.8313 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7997 - prc: 0.4221 - val_loss: 0.0250 - val_cross entropy: 0.3062 - val_mean_squared_error: 0.0863 - val_root_mean_squared_error: 0.2938 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.8000 - val_prc: 0.3152\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0372 - cross entropy: 0.3922 - mean_squared_error: 0.1228 - root_mean_squared_error: 0.3504 - tp: 2.0000 - fp: 2.0000 - tn: 417.0000 - fn: 83.0000 - binary_accuracy: 0.8313 - f1_score: 0.0449 - precision: 0.5000 - recall: 0.0235 - auc: 0.7866 - prc: 0.3695 - val_loss: 0.0497 - val_cross entropy: 0.4601 - val_mean_squared_error: 0.1418 - val_root_mean_squared_error: 0.3766 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7467 - val_prc: 0.2325\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0361 - cross entropy: 0.4021 - mean_squared_error: 0.1258 - root_mean_squared_error: 0.3547 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 419.0000 - fn: 85.0000 - binary_accuracy: 0.8313 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.7921 - prc: 0.3255 - val_loss: 0.0276 - val_cross entropy: 0.3463 - val_mean_squared_error: 0.0988 - val_root_mean_squared_error: 0.3144 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6967 - val_prc: 0.1876\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0325 - cross entropy: 0.3695 - mean_squared_error: 0.1154 - root_mean_squared_error: 0.3396 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 419.0000 - fn: 85.0000 - binary_accuracy: 0.8313 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8304 - prc: 0.4117 - val_loss: 0.0277 - val_cross entropy: 0.3577 - val_mean_squared_error: 0.1017 - val_root_mean_squared_error: 0.3188 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7117 - val_prc: 0.2069\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0295 - cross entropy: 0.3738 - mean_squared_error: 0.1140 - root_mean_squared_error: 0.3376 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 419.0000 - fn: 85.0000 - binary_accuracy: 0.8313 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8477 - prc: 0.4576 - val_loss: 0.0291 - val_cross entropy: 0.3672 - val_mean_squared_error: 0.1052 - val_root_mean_squared_error: 0.3243 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7233 - val_prc: 0.2248\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0293 - cross entropy: 0.3752 - mean_squared_error: 0.1149 - root_mean_squared_error: 0.3390 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 419.0000 - fn: 85.0000 - binary_accuracy: 0.8313 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8471 - prc: 0.4415 - val_loss: 0.0380 - val_cross entropy: 0.4165 - val_mean_squared_error: 0.1236 - val_root_mean_squared_error: 0.3516 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7183 - val_prc: 0.2339\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0279 - cross entropy: 0.3655 - mean_squared_error: 0.1099 - root_mean_squared_error: 0.3315 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 419.0000 - fn: 85.0000 - binary_accuracy: 0.8313 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8653 - prc: 0.5136 - val_loss: 0.0368 - val_cross entropy: 0.4176 - val_mean_squared_error: 0.1234 - val_root_mean_squared_error: 0.3513 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7400 - val_prc: 0.3126\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0271 - cross entropy: 0.3580 - mean_squared_error: 0.1076 - root_mean_squared_error: 0.3280 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 419.0000 - fn: 85.0000 - binary_accuracy: 0.8313 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8769 - prc: 0.5356 - val_loss: 0.0385 - val_cross entropy: 0.4098 - val_mean_squared_error: 0.1210 - val_root_mean_squared_error: 0.3478 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7083 - val_prc: 0.2273\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0263 - cross entropy: 0.3548 - mean_squared_error: 0.1066 - root_mean_squared_error: 0.3265 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 419.0000 - fn: 85.0000 - binary_accuracy: 0.8313 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8869 - prc: 0.5503 - val_loss: 0.0331 - val_cross entropy: 0.3867 - val_mean_squared_error: 0.1109 - val_root_mean_squared_error: 0.3330 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7617 - val_prc: 0.3170\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0317 - cross entropy: 0.3795 - mean_squared_error: 0.1151 - root_mean_squared_error: 0.3393 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 419.0000 - fn: 85.0000 - binary_accuracy: 0.8313 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8193 - prc: 0.4321 - val_loss: 0.0304 - val_cross entropy: 0.3882 - val_mean_squared_error: 0.1110 - val_root_mean_squared_error: 0.3332 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7617 - val_prc: 0.2748\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0293 - cross entropy: 0.3527 - mean_squared_error: 0.1058 - root_mean_squared_error: 0.3252 - tp: 7.0000 - fp: 4.0000 - tn: 415.0000 - fn: 78.0000 - binary_accuracy: 0.8373 - f1_score: 0.1458 - precision: 0.6364 - recall: 0.0824 - auc: 0.8495 - prc: 0.5263 - val_loss: 0.0357 - val_cross entropy: 0.3815 - val_mean_squared_error: 0.1108 - val_root_mean_squared_error: 0.3328 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6933 - val_prc: 0.2484\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0272 - cross entropy: 0.3672 - mean_squared_error: 0.1106 - root_mean_squared_error: 0.3325 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 419.0000 - fn: 85.0000 - binary_accuracy: 0.8313 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8771 - prc: 0.5624 - val_loss: 0.0371 - val_cross entropy: 0.4395 - val_mean_squared_error: 0.1309 - val_root_mean_squared_error: 0.3618 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6933 - val_prc: 0.1895\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.0282 - cross entropy: 0.3695 - mean_squared_error: 0.1123 - root_mean_squared_error: 0.3352 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 419.0000 - fn: 85.0000 - binary_accuracy: 0.8313 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8606 - prc: 0.4875 - val_loss: 0.0335 - val_cross entropy: 0.3833 - val_mean_squared_error: 0.1103 - val_root_mean_squared_error: 0.3321 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7850 - val_prc: 0.2995\n",
      "Epoch 25/100\n",
      " 8/16 [==============>...............] - ETA: 0s - loss: 0.0221 - cross entropy: 0.3177 - mean_squared_error: 0.0893 - root_mean_squared_error: 0.2988 - tp: 10.0000 - fp: 4.0000 - tn: 210.0000 - fn: 32.0000 - binary_accuracy: 0.8594 - f1_score: 0.3571 - precision: 0.7143 - recall: 0.2381 - auc: 0.9013 - prc: 0.6544                 "
     ]
    }
   ],
   "source": [
    "# Begin the search\n",
    "EPOCHS = 100\n",
    "\n",
    "tuner.search(\n",
    "    input_train_scaled,\n",
    "    label_train_scaled,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "# Query the tuner object to grab the best models\n",
    "models = tuner.get_best_models(num_models=5)\n",
    "\n",
    "# Here is the best model from the tuner\n",
    "best_model = models[0]\n",
    "print(best_model.summary())\n",
    "\n",
    "# Get the top hyperparameters.\n",
    "best_hps = tuner.get_best_hyperparameters()\n",
    "\n",
    "# Build the model with the best hyperparameters.\n",
    "# Assuming your MyHyperModel has a build method that takes a hyperparameter object.\n",
    "model = MyHyperModel().build(best_hps[0])\n",
    "\n",
    "# Save the best model (architecture and weights)\n",
    "\n",
    "save_dir = \"/glade/derecho/scratch/rmandava/AEW_time_location_files/models\"\n",
    "model_save_path = os.path.join(save_dir, model_save_name)\n",
    "\n",
    "model.save(model_save_path)\n",
    "\n",
    "\n",
    "# Retrain using \"best\" model hyperparameters\n",
    "history = model.fit(\n",
    "    input_train_scaled,\n",
    "    label_train_scaled,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.1,\n",
    "    batch_size= 32,\n",
    "    # callbacks=keras.callbacks.EarlyStopping('val_loss', patience=3),\n",
    "    shuffle=True,\n",
    "    class_weight=class_weight,  # Ensure 'class_weight' is defined in your scope\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "results = model.evaluate(input_test_scaled, label_test_scaled, batch_size=label_test_scaled.shape[0])\n",
    "print(results)\n",
    "\n",
    "# Generate predictions (probabilities—the output of the last layer)\n",
    "predictions = model.predict(input_test_scaled)\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "\n",
    "# Compute confusion matrix elements\n",
    "tn, fp, fn, tp = sklearn.metrics.confusion_matrix(\n",
    "    label_test_scaled, np.round(predictions)\n",
    ").ravel()\n",
    "print(\"tn:\", tn)\n",
    "print(\"fp:\", fp)\n",
    "print(\"fn:\", fn)\n",
    "print(\"tp:\", tp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8883c1-4450-444a-88a5-18ed3d94d588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# —— Ensemble top K models —— \n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# 1) Grab the top K hyperparameters\n",
    "top_hps = tuner.get_best_hyperparameters(num_trials=5)\n",
    "\n",
    "# 2) Rebuild each best model\n",
    "models = [ MyHyperModel().build(hp) for hp in top_hps ]\n",
    "# (Or, if you saved weights per trial, load them here onto each model.)\n",
    "\n",
    "# 3) Run each model on the test set\n",
    "all_preds = np.stack([m.predict(input_test_scaled).flatten() for m in models], axis=0)\n",
    "\n",
    "# 4) Average their probabilities & threshold\n",
    "ensemble_probs = all_preds.mean(axis=0)\n",
    "ensemble_preds = (ensemble_probs >= 0.5).astype(int)\n",
    "\n",
    "# 5) Compute & print ensemble F1\n",
    "print(\"Ensemble F1:\", f1_score(label_test_scaled, ensemble_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d463e2-12e7-474b-95c1-ae20c22a093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = np.round(predictions.flatten())\n",
    "true_labels = label_test_scaled.flatten()\n",
    "false_neg_idx = np.where((true_labels == 1) & (pred_labels == 0))[0]\n",
    "false_pos_idx = np.where((true_labels == 0) & (pred_labels == 1))[0]\n",
    "true_pos_idx = np.where((true_labels == 1) & (pred_labels == 1))[0]\n",
    "true_neg_idx = np.where((true_labels == 0) & (pred_labels == 0))[0]\n",
    "\n",
    "print(\"Number of false negatives:\", len(false_neg_idx))\n",
    "print(\"Number of false positives:\", len(false_pos_idx))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ddefd-a847-4a5c-a028-ca25e1e1d87c",
   "metadata": {
    "papermill": {
     "duration": 0.042059,
     "end_time": "2025-03-05T20:46:02.526276",
     "exception": false,
     "start_time": "2025-03-05T20:46:02.484217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming lat_test and lon_test are arrays with shape (num_samples, height, width)\n",
    "#central_lat = lat_test[:, lat_test.shape[1]//2, lat_test.shape[2]//2]\n",
    "#central_lon = lon_test[:, lon_test.shape[1]//2, lon_test.shape[2]//2]\n",
    "\n",
    "#lats_false_neg = central_lat[false_neg_idx]\n",
    "#lons_false_neg = central_lon[false_neg_idx]\n",
    "\n",
    "#lats_false_pos = central_lat[false_pos_idx]\n",
    "#lons_false_pos = central_lon[false_pos_idx]\n",
    "# Assuming lat_test and lon_test are 1D arrays with one coordinate per sample\n",
    "lat_false_neg = lat_test[false_neg_idx]\n",
    "lon_false_neg = lon_test[false_neg_idx]\n",
    "\n",
    "lat_false_pos = lat_test[false_pos_idx]\n",
    "lon_false_pos = lon_test[false_pos_idx]\n",
    "\n",
    "lat_true_pos = lat_test[true_pos_idx]\n",
    "lon_true_pos = lon_test[true_pos_idx]\n",
    "\n",
    "lat_true_neg = lat_test[true_neg_idx]\n",
    "lon_true_neg = lon_test[true_neg_idx]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798796b5-b155-48c8-bd46-4c1a1f5fedba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique latitudes in test set:\", np.unique(lat_test))\n",
    "print(\"Unique longitudes in test set:\", np.unique(lon_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0f4581-4248-48f1-bd51-5d4bb476f030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(lon_false_neg, lat_false_neg, marker='x', color='red', label='False Negatives')\n",
    "plt.scatter(lon_false_pos, lat_false_pos, marker='o', color='blue', label='False Positives')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Geographic Distribution of Misclassifications')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f3eb33-d853-48b3-8416-46e48cd99444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512e75d1-8ea6-4a7e-90c9-aaf6ad6d5a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "\n",
    "\n",
    "lon_min, lon_max = np.min(lon_test)-15, np.max(lon_test)+15\n",
    "lat_min, lat_max = np.min(lat_test)-15, np.max(lat_test)+15\n",
    "ax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Add map features\n",
    "ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':', linewidth=0.5)\n",
    "ax.add_feature(cfeature.LAND, edgecolor='black', facecolor='lightgray')\n",
    "ax.add_feature(cfeature.OCEAN, facecolor='lightblue')\n",
    "\n",
    "# Plot misclassified points:\n",
    "ax.scatter(lon_false_neg, lat_false_neg, color='red', marker='x', s=100,\n",
    "           transform=ccrs.PlateCarree(), label='False Negatives')\n",
    "ax.scatter(lon_false_pos, lat_false_pos, color='blue', marker='o', s=100,\n",
    "           transform=ccrs.PlateCarree(), label='False Positives')\n",
    "\n",
    "plt.title(\"Misclassified Test Samples on Map\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4561912c-9788-442a-a527-f125871dbce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Set map extent (adjust margins as desired)\n",
    "lon_min, lon_max = np.min(lon_test) - 10, np.max(lon_test) + 10\n",
    "lat_min, lat_max = np.min(lat_test) - 10, np.max(lat_test) + 10\n",
    "ax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Add map features\n",
    "ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':', linewidth=0.5)\n",
    "ax.add_feature(cfeature.LAND, facecolor='lightgray', edgecolor='black')\n",
    "ax.add_feature(cfeature.OCEAN, facecolor='lightblue')\n",
    "# Plot misclassified points\n",
    "ax.scatter(lon_false_neg, lat_false_neg, color='red', marker='x', s=100,\n",
    "           transform=ccrs.PlateCarree(), label='False Negatives')\n",
    "ax.scatter(lon_false_pos, lat_false_pos, color='blue', marker='o', s=100,\n",
    "           transform=ccrs.PlateCarree(), label='False Positives')\n",
    "\n",
    "\n",
    "\n",
    "# Plot correctly classified points\n",
    "ax.scatter(lon_true_pos, lat_true_pos, color='green', marker='^', s=100,\n",
    "           transform=ccrs.PlateCarree(), label='True Positives')\n",
    "ax.scatter(lon_true_neg, lat_true_neg, color='orange', marker='s', s=100,\n",
    "           transform=ccrs.PlateCarree(), label='True Negatives')\n",
    "\n",
    "plt.title(\"Test Set Classification Results on Map\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe393f-a6c9-440d-b648-78010a1d0ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total samples:\", len(sample_lat))\n",
    "print(\"Unique latitudes:\", len(np.unique(sample_lat)))\n",
    "print(\"Unique longitudes:\", len(np.unique(sample_lon)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e932dda-37e1-4158-8655-7d2a041a1a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First 10 latitudes:\", sample_lat[:10])\n",
    "print(\"First 10 longitudes:\", sample_lon[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe686778-d179-4dec-8b2e-472700d70e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(sample_lon, sample_lat, c='green', marker='o')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Per-Sample Weighted Centroid Coordinates')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5563a928-7d7f-402d-94e1-63dd0cff0982",
   "metadata": {
    "papermill": {
     "duration": 13.848113,
     "end_time": "2025-03-05T20:46:16.415723",
     "exception": false,
     "start_time": "2025-03-05T20:46:02.567610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Permutation Importance ---\n",
    "# Evaluate baseline performance using your loss metric (here, the custom f1_loss_sigmoid)\n",
    "# model.metrics_names gives a list where index 0 is 'loss'\n",
    "baseline_results = model.evaluate(input_test_scaled, label_test_scaled,\n",
    "                                  batch_size=label_test_scaled.shape[0],\n",
    "                                  verbose=0)\n",
    "baseline_loss = baseline_results[model.metrics_names.index('loss')]\n",
    "print(\"Baseline loss:\", baseline_loss)\n",
    "\n",
    "# Set the number of repetitions for averaging\n",
    "n_repeats = 5\n",
    "n_features = input_test_scaled.shape[-1]\n",
    "permutation_importances = np.zeros(n_features)\n",
    "\n",
    "# Loop over each feature (channel)\n",
    "for feature_idx in range(n_features):\n",
    "    permuted_losses = []\n",
    "    for _ in range(n_repeats):\n",
    "        # Copy the test set to avoid modifying the original\n",
    "        X_permuted = np.copy(input_test_scaled)\n",
    "        # Permute the values of the selected feature across samples\n",
    "        perm = np.random.permutation(X_permuted.shape[0])\n",
    "        X_permuted[:, :, :, feature_idx] = X_permuted[perm, :, :, feature_idx]\n",
    "        \n",
    "        # Evaluate the model on the permuted test set\n",
    "        permuted_results = model.evaluate(X_permuted, label_test_scaled,\n",
    "                                          batch_size=label_test_scaled.shape[0],\n",
    "                                          verbose=0)\n",
    "        permuted_loss = permuted_results[model.metrics_names.index('loss')]\n",
    "        permuted_losses.append(permuted_loss)\n",
    "    \n",
    "    avg_permuted_loss = np.mean(permuted_losses)\n",
    "    # The difference between the permuted loss and baseline loss indicates feature importance:\n",
    "    # A larger increase means the feature is more important.\n",
    "    permutation_importances[feature_idx] = avg_permuted_loss - baseline_loss\n",
    "    print(f\"Feature {feature_idx} - Increase in Loss: {permutation_importances[feature_idx]}\")\n",
    "\n",
    "print(\"Permutation Importances (increase in loss) for all features:\", permutation_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23e4f15-e79e-4040-b328-2096906415f9",
   "metadata": {
    "papermill": {
     "duration": 0.467125,
     "end_time": "2025-03-05T20:46:16.925109",
     "exception": false,
     "start_time": "2025-03-05T20:46:16.457984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Suppose permutation_importances is the numpy array you printed:\n",
    "# e.g., [0.0820, 0.0824, 0.0815, 0.0864, 0.0121, ...]\n",
    "\n",
    "feature_indices = np.arange(len(permutation_importances))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(feature_indices, permutation_importances)\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Increase in Loss (Permutation Importance)\")\n",
    "plt.title(\"Permutation Importance by Feature\")\n",
    "plt.xticks(feature_indices, [f\"F{i}\" for i in feature_indices], rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1765f8b0-69ed-4917-82f7-f15ef12cc837",
   "metadata": {
    "papermill": {
     "duration": 0.043518,
     "end_time": "2025-03-05T20:46:17.012058",
     "exception": false,
     "start_time": "2025-03-05T20:46:16.968540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_index = 0\n",
    "sample_input = input_test_scaled[sample_index:sample_index+1]\n",
    "\n",
    "# Compute the saliency map for the selected test sample\n",
    "saliency_map = compute_saliency_map(model, sample_input)\n",
    "\n",
    "# Plot the saliency map\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(saliency_map, cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.title(f\"Saliency Map for Sample Index {sample_index}\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa54c93-f08a-46e2-a610-95424eb79806",
   "metadata": {
    "papermill": {
     "duration": 0.042628,
     "end_time": "2025-03-05T20:46:17.098037",
     "exception": false,
     "start_time": "2025-03-05T20:46:17.055409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_index = 1\n",
    "sample_input = input_test_scaled[sample_index:sample_index+1]\n",
    "\n",
    "# Compute saliency maps per channel for the selected input sample\n",
    "saliency_maps, channel_importance = compute_saliency_per_channel(model, sample_input)\n",
    "\n",
    "# Plot the saliency maps for each channel\n",
    "num_channels = saliency_maps.shape[-1]\n",
    "cols = 5  # Set number of columns for plotting\n",
    "rows = int(np.ceil(num_channels / cols))\n",
    "plt.figure(figsize=(15, rows * 3))\n",
    "for c in range(num_channels):\n",
    "    plt.subplot(rows, cols, c + 1)\n",
    "    plt.imshow(saliency_maps[:, :, c], cmap='hot')\n",
    "    plt.title(f'Channel {c}\\nMean: {channel_importance[c]:.4f}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print aggregated channel importance values\n",
    "print(\"Channel importance (mean saliency per channel):\")\n",
    "for c, imp in enumerate(channel_importance):\n",
    "    print(f\"Channel {c}: {imp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec3979-63a3-4211-9475-3445ad530009",
   "metadata": {
    "papermill": {
     "duration": 0.043201,
     "end_time": "2025-03-05T20:46:17.184585",
     "exception": false,
     "start_time": "2025-03-05T20:46:17.141384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Compute per‑sample, per‑channel saliency importances\n",
    "all_imps = []\n",
    "for x in input_test_scaled:             # each x has shape (32,32,channels)\n",
    "    _, imp = compute_saliency_per_channel(best_model, x[np.newaxis,...])\n",
    "    all_imps.append(imp)               # imp.shape == (channels,)\n",
    "all_imps = np.stack(all_imps)          # shape (N_samples, channels)\n",
    "\n",
    "# 2) Average importance across samples\n",
    "mean_imp = all_imps.mean(axis=0)       # shape (channels,)\n",
    "\n",
    "# 3) Build a DataFrame mapping feature→plevel→importance\n",
    "df = pd.DataFrame({\n",
    "    \"feature\": var_list,\n",
    "    \"plevel\": plevel_list,\n",
    "    \"importance\": mean_imp\n",
    "})\n",
    "\n",
    "# 4) Group by pressure level and plot\n",
    "grouped = df.groupby(\"plevel\")[\"importance\"].mean().reset_index()\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(grouped[\"plevel\"].astype(str), grouped[\"importance\"])\n",
    "plt.xlabel(\"Pressure level (hPa or False=surface)\")\n",
    "plt.ylabel(\"Mean saliency importance\")\n",
    "plt.title(\"Average feature importance by pressure level\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f421b06b-75d2-4e23-beb0-dbb5f6c6240f",
   "metadata": {
    "papermill": {
     "duration": 0.041979,
     "end_time": "2025-03-05T20:46:17.268705",
     "exception": false,
     "start_time": "2025-03-05T20:46:17.226726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1) Print out full test‐set metrics by name\n",
    "test_results = model.evaluate(\n",
    "    input_test_scaled,\n",
    "    label_test_scaled,\n",
    "    batch_size=label_test_scaled.shape[0],\n",
    "    verbose=0\n",
    ")\n",
    "print(dict(zip(model.metrics_names, test_results)))\n",
    "\n",
    "# 2) Peek at the end of your training history for F1 improvements\n",
    "import pandas as pd\n",
    "hist_df = pd.DataFrame(history.history)\n",
    "print(hist_df[['f1_score','val_f1_score']].tail())\n",
    "\n",
    "# 3) (Optional) Plot train vs. val F1 over epochs\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist_df['f1_score'],    label='train F1')\n",
    "plt.plot(hist_df['val_f1_score'],label='val F1')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.title('Focal‐Loss Training F1 Curves')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b46d58-ad34-4817-a519-c9152372b7d4",
   "metadata": {
    "papermill": {
     "duration": 0.047731,
     "end_time": "2025-03-05T20:46:17.359595",
     "exception": false,
     "start_time": "2025-03-05T20:46:17.311864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace your rebuild logic:\n",
    "# top_hps = tuner.get_best_hyperparameters(num_trials=5)\n",
    "# models  = [MyHyperModel().build(hp) for hp in top_hps]\n",
    "\n",
    "# With this single line:\n",
    "models = tuner.get_best_models(num_models=5)\n",
    "\n",
    "# Then stack and average as before:\n",
    "all_preds     = np.stack([m.predict(input_test_scaled).flatten() for m in models], axis=0)\n",
    "ensemble_probs= all_preds.mean(axis=0)\n",
    "ensemble_preds= (ensemble_probs >= 0.5).astype(int)\n",
    "print(\"Ensemble F1:\", f1_score(label_test_scaled, ensemble_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae3ef17-24f6-4569-8095-2bdee2e67245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7035.678234,
   "end_time": "2025-03-05T20:46:20.341478",
   "environment_variables": {},
   "exception": null,
   "input_path": "Hurricaneoriginal(300).ipynb",
   "output_path": "output_notebook_var(second)0.ipynb",
   "parameters": {
    "aew_subset": "12hr_before",
    "model_save_name": "best_model_var0.keras",
    "plevel_list": [
     false,
     false,
     300,
     false,
     false,
     false,
     300,
     300,
     850,
     300,
     850,
     false,
     false,
     false,
     false,
     300,
     850,
     false,
     300,
     850,
     300,
     850,
     300,
     300,
     850
    ],
    "tuner_project_name": "tuner_run(second)_0",
    "var_list": [
     "cape",
     "crr",
     "d",
     "ie",
     "ishf",
     "lsrr",
     "pv",
     "q",
     "q",
     "r",
     "r",
     "sp",
     "sstk",
     "tcw",
     "tcwv",
     "t",
     "t",
     "ttr",
     "u",
     "u",
     "v",
     "v",
     "vo",
     "w",
     "w"
    ]
   },
   "start_time": "2025-03-05T18:49:04.663244",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
