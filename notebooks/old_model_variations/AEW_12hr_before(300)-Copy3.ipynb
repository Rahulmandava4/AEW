{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2203c2-df40-428c-ae4c-71b20ba050c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8bf88bb8-756d-47cb-ad74-88b3b302a462",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.022322,
     "end_time": "2025-03-05T18:49:06.250982",
     "exception": false,
     "start_time": "2025-03-05T18:49:06.228660",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "Parameters",
     "parameters",
     "  Parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "var_list = [\"default1\", \"default2\"]\n",
    "plevel_list = [False, 300]\n",
    "aew_subset = \"default_subset\"\n",
    "model_save_name = \"default_modelbase1.keras\"\n",
    "tuner_project_name = \"default_tuner_run1\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6798e3f",
   "metadata": {
    "papermill": {
     "duration": 0.012789,
     "end_time": "2025-03-05T18:49:06.269446",
     "exception": false,
     "start_time": "2025-03-05T18:49:06.256657",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# Parameters\n",
    "var_list = [\"cape\", \"crr\", \"d\", \"ie\", \"ishf\", \"lsrr\", \"pv\", \"q\",\"r\", \"sp\", \"sstk\", \"tcw\", \"tcwv\", \"t\", \"ttr\", \"u\",\"v\", \"vo\",\"w\"] #ERA5 variables\n",
    "\n",
    "\n",
    "\n",
    "plevel_list = [False, False,300, False, False, False, 300, 300, 300,False, False, False, False, 300,  False, 300,300  ,  300, 300] #pressure levels of variables\n",
    "\n",
    "aew_subset = \"12hr_before\"\n",
    "model_save_name = \"best_model_var(300)1234.keras\"\n",
    "tuner_project_name = \"tuner_run(300)1234\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe4b521-b034-49f0-a1ac-d961fe639066",
   "metadata": {
    "papermill": {
     "duration": 0.005028,
     "end_time": "2025-03-05T18:49:06.279538",
     "exception": false,
     "start_time": "2025-03-05T18:49:06.274510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3d1aa354-4561-4486-9f61-73f2c1c81a31",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 13.973479,
     "end_time": "2025-03-05T18:49:20.258119",
     "exception": false,
     "start_time": "2025-03-05T18:49:06.284640",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import sklearn.model_selection\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras import layers\n",
    "\n",
    "import keras_tuner\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "keras.utils.set_random_seed(812)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "32c4826e-dbc1-45c0-aca0-bc38d0254f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    \"\"\"Focal Loss for binary classification.\"\"\"\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        # Clip to prevent NaNs \n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        bce = tf.keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        modulating_factor = tf.pow(1.0 - p_t, gamma)\n",
    "        alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        return alpha_factor * modulating_factor * bce\n",
    "    return loss_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c74c2082-8917-4d0d-a2d7-b39f8548bcbb",
   "metadata": {
    "papermill": {
     "duration": 0.014754,
     "end_time": "2025-03-05T18:49:20.282885",
     "exception": false,
     "start_time": "2025-03-05T18:49:20.268131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#functions to calculate f1 score as loss function\n",
    "\n",
    "def f1_loss_sigmoid(y_true, y_pred):\n",
    "    \"\"\"\n",
    "\n",
    "    F1 metric for sigmoid output and integer encoded labels.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # compute tp, fp, and fn\n",
    "\n",
    "    tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n",
    "\n",
    "    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\n",
    "\n",
    "    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\n",
    "\n",
    "\n",
    "    # precision (tp / (tp + fp))\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "\n",
    "   # recall (tp / (tp + fn))\n",
    "\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "\n",
    "# harmonic mean of precision and recall\n",
    "\n",
    "    f1 = (2 * p * r) / (p + r + K.epsilon())\n",
    "\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "\n",
    "    return 1 - K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "58fe83a8-b5b0-4a10-81f6-a5b9277e5a3a",
   "metadata": {
    "papermill": {
     "duration": 0.012029,
     "end_time": "2025-03-05T18:49:20.300625",
     "exception": false,
     "start_time": "2025-03-05T18:49:20.288596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f1_loss_onehot(y_true, y_pred):\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "   F1 metric for two-class output and one-hot encoded labels.\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "   # compute tp, fp, and fn\n",
    "\n",
    "   tp = K.sum(K.cast(y_true[:, 1] * y_pred[:, 1], 'float'), axis=0)\n",
    "\n",
    "   fp = K.sum(K.cast((1 - y_true[:, 1]) * y_pred[:, 1], 'float'), axis=0)\n",
    "\n",
    "   fn = K.sum(K.cast(y_true[:, 0] * (1 - y_pred[:, 0]), 'float'), axis=0)\n",
    "\n",
    "\n",
    "   # precision (tp / (tp + fp))\n",
    "\n",
    "   p = tp / (tp + fp + K.epsilon())\n",
    "\n",
    "   # recall (tp / (tp + fn))\n",
    "\n",
    "   r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "\n",
    "   # harmonic mean of precision and recall\n",
    "\n",
    "   f1 = (2 * p * r) / (p + r + K.epsilon())\n",
    "\n",
    "   f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "\n",
    "   return 1 - K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "82e6df90-377d-4c27-ac1d-156b3bc34623",
   "metadata": {
    "papermill": {
     "duration": 0.012229,
     "end_time": "2025-03-05T18:49:20.319927",
     "exception": false,
     "start_time": "2025-03-05T18:49:20.307698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def add_dim(ds):\n",
    "    # Extract the source file name from the dataset's encoding.\n",
    "    fname = ds.encoding.get('source', '')\n",
    "    # Use a regex to capture the central latitude and longitude from the filename.\n",
    "    m = re.search(r'_(\\-?\\d+\\.\\d+)_(-?\\d+\\.\\d+)\\.nc$', fname)\n",
    "    if m:\n",
    "        lat_center = float(m.group(1))\n",
    "        lon_center = float(m.group(2))\n",
    "        # Assign the central coordinates and the file name as new coordinates.\n",
    "        ds = ds.assign_coords(lat_center=lat_center, lon_center=lon_center, file_name=fname)\n",
    "    else:\n",
    "        print(\"File name does not match expected pattern:\", fname)\n",
    "    \n",
    "    # Expand dims to add the 'sample' dimension and drop unnecessary variables.\n",
    "    return ds.assign_coords({\"sample\": 1}).expand_dims(dim={\"sample\": 1}).drop_vars(\"utc_date\").drop_vars(\"latitude\").drop_vars(\"longitude\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec3de133-3ed1-4df3-9444-a58e88716284",
   "metadata": {
    "papermill": {
     "duration": 0.017203,
     "end_time": "2025-03-05T18:49:20.344038",
     "exception": false,
     "start_time": "2025-03-05T18:49:20.326835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "def open_files_zarr(list_of_vars, aew_subset=\"12hr_before\",\n",
    "                    directory=\"/glade/derecho/scratch/rmandava/AEW_time_location_files/\",\n",
    "                    plevel_list=None, zarr_store_path=\"zarr_data\"):\n",
    "    \"\"\"\n",
    "    Opens ERA5 NetCDF files for the given variables. For each variable (or pressure-level variant),\n",
    "    it checks if a corresponding Zarr store exists in 'zarr_store_path'. If so, it loads the dataset\n",
    "    from the Zarr store; if not, it opens the NetCDF files, preprocesses them, saves them to Zarr,\n",
    "    and then returns the dataset.\n",
    "    \"\"\"\n",
    "    # Create the zarr_store_path directory if it doesn't exist.\n",
    "    if not os.path.exists(zarr_store_path):\n",
    "        os.makedirs(zarr_store_path)\n",
    "    \n",
    "    datas = {}\n",
    "    for num, var in enumerate(list_of_vars):\n",
    "        # Determine the key and filename based on whether a pressure level is specified.\n",
    "        if plevel_list:\n",
    "            if plevel_list[num]:\n",
    "                key = f\"{var}_{int(plevel_list[num])}\"\n",
    "                file_pattern = f'{directory}/{var}/aew_{aew_subset}_{int(plevel_list[num])}_*.nc'\n",
    "            else:\n",
    "                key = var\n",
    "                file_pattern = f'{directory}/{var}/aew_{aew_subset}_*.nc'\n",
    "        else:\n",
    "            key = var\n",
    "            file_pattern = f'{directory}/{var}/aew_{aew_subset}_*.nc'\n",
    "        \n",
    "        # Define the zarr path for this variable.\n",
    "        zarr_path = os.path.join(zarr_store_path, f\"{key}.zarr\")\n",
    "        \n",
    "        # If the Zarr dataset exists, load from it; otherwise, create it.\n",
    "        if os.path.exists(zarr_path):\n",
    "            print(f\"Loading {key} from Zarr store.\")\n",
    "            ds = xr.open_zarr(zarr_path)\n",
    "        else:\n",
    "            print(f\"Creating Zarr store for {key} from NetCDF files.\")\n",
    "            ds = xr.open_mfdataset(\n",
    "                file_pattern,\n",
    "                preprocess=add_dim,\n",
    "                concat_dim=\"sample\",\n",
    "                combine=\"nested\",\n",
    "            )\n",
    "            ds.to_zarr(zarr_path, mode=\"w\")\n",
    "        datas[key] = ds\n",
    "    \n",
    "    return datas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c80e6-2437-4626-be0b-4d82bebb4f73",
   "metadata": {
    "papermill": {
     "duration": 0.015473,
     "end_time": "2025-03-05T18:49:20.366598",
     "exception": false,
     "start_time": "2025-03-05T18:49:20.351125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ad1f231e-abc1-49e7-9bab-09ab1f3f4ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_load_concat(data_dictionary):\n",
    "    # Instead of eagerly converting to NumPy arrays, keep the datasets as xarray objects.\n",
    "    transposed = {}\n",
    "    for key, ds in data_dictionary.items():\n",
    "        var_name = key.split('_')[0].upper()\n",
    "        # Do lazy transpose and add a 'features' dimension\n",
    "        transposed[key] = ds[var_name].expand_dims('features').transpose('sample', 'latitude', 'longitude', 'features')\n",
    "    # Concatenate along the new 'features' dimension (if multiple variables exist)\n",
    "    if len(transposed) > 1:\n",
    "        data = xr.concat(list(transposed.values()), dim='features',coords='minimal',compat='override')\n",
    "    else:\n",
    "        data = list(transposed.values())[0]\n",
    "    # Use the coordinates (lat_center, lon_center) from one of the datasets.\n",
    "    # They remain lazy and are not computed until needed.\n",
    "    first_key = next(iter(data_dictionary))\n",
    "    lat_center = data_dictionary[first_key]['lat_center']\n",
    "    lon_center = data_dictionary[first_key]['lon_center']\n",
    "    label = data_dictionary[first_key]['label']\n",
    "    return data, label, lat_center, lon_center\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bec2717c-8176-447f-82ea-5b15e038ace1",
   "metadata": {
    "papermill": {
     "duration": 0.010767,
     "end_time": "2025-03-05T18:49:20.384143",
     "exception": false,
     "start_time": "2025-03-05T18:49:20.373376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def omit_nans(data, label, lat, lon):\n",
    "    # If data is an xarray DataArray, convert it to a NumPy array\n",
    "    if hasattr(data, 'values'):\n",
    "        data = data.values\n",
    "    maskarray = np.full(data.shape[0], True)\n",
    "    # Find indices where NaNs occur\n",
    "    masker = np.unique(np.argwhere(np.isnan(data))[:, 0])\n",
    "    maskarray[masker] = False\n",
    "\n",
    "    traindata = data[maskarray, ...]\n",
    "    trainlabel = label[maskarray]\n",
    "    lat_filtered = lat[maskarray]\n",
    "    lon_filtered = lon[maskarray]\n",
    "    return traindata, trainlabel, lat_filtered, lon_filtered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d02dbe1a-f1ba-4f9b-8fe0-d9126814ae62",
   "metadata": {
    "papermill": {
     "duration": 0.010866,
     "end_time": "2025-03-05T18:49:20.400498",
     "exception": false,
     "start_time": "2025-03-05T18:49:20.389632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def zscore(data):\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  Rescaling the data using zscore (mean/std).\n",
    "\n",
    "  Each variable gets scaled independently from others.\n",
    "\n",
    "  Note that we will need to remove test data for formal training.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  for i in range(0, data_.shape[-1]):\n",
    "\n",
    "      data_[:, :, :, i] = (\n",
    "\n",
    "               data_[:, :, :, i] - np.nanmean(\n",
    "\n",
    "                     data_[:, :, :, i])) / np.nanstd(data_[:, :, :, i])\n",
    "\n",
    "  return data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "332bc130-a4e7-465c-a135-687f543db181",
   "metadata": {
    "papermill": {
     "duration": 0.011066,
     "end_time": "2025-03-05T18:49:20.416849",
     "exception": false,
     "start_time": "2025-03-05T18:49:20.405783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def minmax(data):\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "   Rescaling the data using min-max.\n",
    "\n",
    "   Each variable gets scaled independently from others.\n",
    "\n",
    "   Note that we will need to remove test data for formal training.\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "   for i in range(0, data_.shape[-1]):\n",
    "\n",
    "          data_[:, :, :, i] = (\n",
    "\n",
    "              data_[:, :, :, i] - np.nanmin(data_[:, :, :, i])\n",
    "\n",
    "          ) / (np.nanmax(data_[:, :, :, i]) - np.nanmin(data_[:, :, :, i]))\n",
    "   return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f652c778-f81b-4c46-bf87-d60487903b73",
   "metadata": {
    "papermill": {
     "duration": 0.010904,
     "end_time": "2025-03-05T18:49:20.433062",
     "exception": false,
     "start_time": "2025-03-05T18:49:20.422158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_split(data, label, split=0.3, seed=0):\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "   Help spliting data randomly for training and testing.\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "   np.random.seed(0)\n",
    "\n",
    "   da_indx = np.random.permutation(data.shape[0])\n",
    "\n",
    "   data = data[da_indx.astype(int)]\n",
    "\n",
    "   label = label[da_indx.astype(int)]\n",
    "\n",
    "   init_range = int(data.shape[0] * (1 - 0.3))\n",
    "\n",
    "   return data[:init_range], label[:init_range], data[init_range:], label[init_range:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c9f970b2-2444-4dba-8e66-a850482d5ea4",
   "metadata": {
    "papermill": {
     "duration": 0.010837,
     "end_time": "2025-03-05T18:49:20.449475",
     "exception": false,
     "start_time": "2025-03-05T18:49:20.438638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pick_loss(loss_string):\n",
    "    \n",
    "\n",
    "    lossdict = {\n",
    "        \"relu\": tf.nn.relu,\n",
    "        \"tanh\": tf.nn.tanh,\n",
    "        \"selu\": tf.nn.selu,\n",
    "        \"sigmoid\": tf.nn.sigmoid,\n",
    "        \"relu6\": tf.nn.relu6,\n",
    "        \"silu\": tf.nn.silu,\n",
    "        \"gelu\": tf.nn.gelu,\n",
    "        \"lrelu\": tf.nn.leaky_relu,\n",
    "    }\n",
    "\n",
    "    return lossdict[loss_string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d3e0c99e-885a-4f64-b057-0056b0ef246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency_map(model, input_sample):\n",
    "    \"\"\"\n",
    "    Compute a saliency map for a given input sample using a gradient-based approach.\n",
    "    \n",
    "    Args:\n",
    "        model (tf.keras.Model): The trained Keras model.\n",
    "        input_sample (numpy array): A single input sample of shape (1, height, width, channels).\n",
    "    \n",
    "    Returns:\n",
    "        saliency (numpy array): The saliency map of shape (height, width).\n",
    "    \"\"\"\n",
    "    # Ensure the model is in inference mode\n",
    "    model.trainable = False\n",
    "    input_tensor = tf.convert_to_tensor(input_sample)\n",
    "    \n",
    "    # Use GradientTape to record operations for automatic differentiation\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Watch the input tensor\n",
    "        tape.watch(input_tensor)\n",
    "        # Get the model's prediction\n",
    "        prediction = model(input_tensor)\n",
    "    \n",
    "    # Compute gradients of the prediction with respect to the input\n",
    "    grads = tape.gradient(prediction, input_tensor)\n",
    "    \n",
    "    # If there are multiple channels, take the maximum absolute gradient across channels\n",
    "    saliency = np.max(np.abs(grads.numpy()), axis=-1)[0]\n",
    "    return saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9776cdc0-38bc-476f-b93d-3753a2eb6803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency_per_channel(model, input_sample):\n",
    "    \"\"\"\n",
    "    Computes the saliency map for each channel of a given input sample.\n",
    "    \n",
    "    Args:\n",
    "        model (tf.keras.Model): The trained Keras model.\n",
    "        input_sample (numpy array): A single input sample with shape (1, H, W, C).\n",
    "        \n",
    "    Returns:\n",
    "        saliency_maps (numpy array): Absolute gradients with shape (H, W, C) for each channel.\n",
    "        channel_importance (numpy array): Mean saliency per channel (shape: (C,)).\n",
    "    \"\"\"\n",
    "    # Set the model to inference mode\n",
    "    model.trainable = False\n",
    "    input_tensor = tf.convert_to_tensor(input_sample)\n",
    "    \n",
    "    # Compute gradients with respect to the input sample using GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(input_tensor)\n",
    "        prediction = model(input_tensor)\n",
    "    \n",
    "    # Calculate gradients: shape (1, H, W, C)\n",
    "    grads = tape.gradient(prediction, input_tensor)\n",
    "    \n",
    "    # Remove the batch dimension: shape becomes (H, W, C)\n",
    "    grads = grads.numpy()[0]\n",
    "    \n",
    "    # Take absolute value to measure importance (magnitude of sensitivity)\n",
    "    saliency_maps = np.abs(grads)\n",
    "    \n",
    "    # Aggregate saliency per channel (e.g., using the mean over spatial dimensions)\n",
    "    channel_importance = np.mean(saliency_maps, axis=(0, 1))\n",
    "    \n",
    "    return saliency_maps, channel_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "81aa5f1b-5513-4496-9fae-a6cde06e1aaa",
   "metadata": {
    "papermill": {
     "duration": 0.00964,
     "end_time": "2025-03-05T18:49:20.464813",
     "exception": false,
     "start_time": "2025-03-05T18:49:20.455173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_of_features = len(var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3490622e-a626-42aa-9cfe-020ee54d21e0",
   "metadata": {
    "papermill": {
     "duration": 758.818091,
     "end_time": "2025-03-05T19:01:59.288567",
     "exception": false,
     "start_time": "2025-03-05T18:49:20.470476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cape from Zarr store.\n",
      "Loading crr from Zarr store.\n",
      "Loading d_300 from Zarr store.\n",
      "Loading ie from Zarr store.\n",
      "Loading ishf from Zarr store.\n",
      "Loading lsrr from Zarr store.\n",
      "Loading pv_300 from Zarr store.\n",
      "Loading q_300 from Zarr store.\n",
      "Loading r_300 from Zarr store.\n",
      "Loading sp from Zarr store.\n",
      "Loading sstk from Zarr store.\n",
      "Loading tcw from Zarr store.\n",
      "Loading tcwv from Zarr store.\n",
      "Loading t_300 from Zarr store.\n",
      "Loading ttr from Zarr store.\n",
      "Loading u_300 from Zarr store.\n",
      "Loading v_300 from Zarr store.\n",
      "Loading vo_300 from Zarr store.\n",
      "Loading w_300 from Zarr store.\n"
     ]
    }
   ],
   "source": [
    "data = open_files_zarr(\n",
    "    list_of_vars=var_list,\n",
    "    aew_subset=aew_subset,\n",
    "    directory=\"/glade/derecho/scratch/rmandava/AEW_time_location_files/\",\n",
    "    plevel_list=plevel_list,\n",
    "    zarr_store_path=\"/glade/derecho/scratch/rmandava/AEW_time_location_files/Project1/zarr\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ddac3e-071b-4796-9ca6-9cb01deeb598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f240d4af-46d1-4cef-a5ab-2e2527a1639f",
   "metadata": {
    "papermill": {
     "duration": 185.616237,
     "end_time": "2025-03-05T19:05:04.925249",
     "exception": false,
     "start_time": "2025-03-05T19:01:59.309012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2750, 32, 32, 19)\n"
     ]
    }
   ],
   "source": [
    "# transpose the data and concat variables\n",
    "\n",
    "#data_, labels_ = transpose_load_concat(data)\n",
    "data_, labels_, sample_lat, sample_lon = transpose_load_concat(data)\n",
    "\n",
    "print(np.shape(data_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a0e4f7b8-f9e0-4131-ae35-97e47bc557d0",
   "metadata": {
    "papermill": {
     "duration": 0.372532,
     "end_time": "2025-03-05T19:05:05.305951",
     "exception": false,
     "start_time": "2025-03-05T19:05:04.933419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check / remove nans\n",
    "\n",
    "data_, labels_, sample_lat, sample_lon = omit_nans(data_, labels_, sample_lat, sample_lon)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0492935e-1f35-446b-ab16-aba0e667857b",
   "metadata": {
    "papermill": {
     "duration": 0.044227,
     "end_time": "2025-03-05T19:05:05.358050",
     "exception": false,
     "start_time": "2025-03-05T19:05:05.313823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(560, 32, 32, 19) (140, 32, 32, 19) (560,) (140,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/u/home/rmandava/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1642: PerformanceWarning: Slicing with an out-of-order index is generating 106 times more chunks\n",
      "  return self.array[key]\n",
      "/glade/u/home/rmandava/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1642: PerformanceWarning: Slicing with an out-of-order index is generating 27 times more chunks\n",
      "  return self.array[key]\n"
     ]
    }
   ],
   "source": [
    "#split train and test set\n",
    "X_train, X_test, y_train, y_test, lat_train, lat_test, lon_train, lon_test = sklearn.model_selection.train_test_split(\n",
    "    data_, labels_, sample_lat, sample_lon, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "print (np.shape(X_train), np.shape(X_test), np.shape(y_train), np.shape(y_test))\n",
    "\n",
    "\n",
    "y_train = np.expand_dims(y_train, axis=1)\n",
    "\n",
    "y_test = np.expand_dims(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "164ee316-7c83-474b-bdb9-7c12ca165e1f",
   "metadata": {
    "papermill": {
     "duration": 0.312834,
     "end_time": "2025-03-05T19:05:05.678612",
     "exception": false,
     "start_time": "2025-03-05T19:05:05.365778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [21]: Scaling code (fixed to prevent data leakage)\n",
    "\n",
    "# Create the scaler object\n",
    "scaler_input = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "# Reshape training data to 2D (samples, features)\n",
    "X_train_tmp = np.reshape(X_train, (-1, len(var_list)))\n",
    "\n",
    "# Fit the scaler ONLY on the training data\n",
    "scaler_input.fit(X_train_tmp)  # <-- Key change: Learn mean/std from training data\n",
    "\n",
    "# Transform BOTH training and test data using the SAME scaler\n",
    "input_train_scaled = scaler_input.transform(X_train_tmp)          # Train: transform only\n",
    "input_test_scaled = scaler_input.transform(                       # Test: transform only\n",
    "    np.reshape(X_test, (-1, len(var_list)))\n",
    ")\n",
    "\n",
    "# Reshape back to original dimensions (samples, height, width, features)\n",
    "input_train_scaled = np.reshape(input_train_scaled, X_train.shape)\n",
    "input_test_scaled = np.reshape(input_test_scaled, X_test.shape)\n",
    "\n",
    "# Labels remain unchanged\n",
    "label_train_scaled = y_train\n",
    "label_test_scaled = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6bed418e-264b-45e6-92ee-b72c2f7a04d7",
   "metadata": {
    "papermill": {
     "duration": 0.014239,
     "end_time": "2025-03-05T19:05:05.701116",
     "exception": false,
     "start_time": "2025-03-05T19:05:05.686877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(560, 32, 32, 19) (560, 1) (140, 32, 32, 19) (140, 1)\n"
     ]
    }
   ],
   "source": [
    "# print the shapes to double check them\n",
    "\n",
    "print(\n",
    "\n",
    "input_train_scaled.shape,\n",
    "\n",
    "label_train_scaled.shape,\n",
    "\n",
    "input_test_scaled.shape,\n",
    "\n",
    "label_test_scaled.shape\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "10597eb7-f8fa-4523-a306-55d5a442d504",
   "metadata": {
    "papermill": {
     "duration": 0.017418,
     "end_time": "2025-03-05T19:05:05.726147",
     "exception": false,
     "start_time": "2025-03-05T19:05:05.708729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive samples in training data: 91 (16.25% of total)\n"
     ]
    }
   ],
   "source": [
    "#generate class weights due to class imbalance issues\n",
    "\n",
    "counts = np.bincount(y_train[:, 0].astype(int))\n",
    "\n",
    "\n",
    "print(\n",
    "\n",
    "\"Number of positive samples in training data: {} ({:.2f}% of total)\".format(\n",
    "\n",
    "counts[1], 100 * float(counts[1]) / len(y_train))\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "557d98a2-07ba-4fa7-8ba2-9ce8baa53b63",
   "metadata": {
    "papermill": {
     "duration": 0.01967,
     "end_time": "2025-03-05T19:05:05.753659",
     "exception": false,
     "start_time": "2025-03-05T19:05:05.733989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.1625, 1: 0.8375}\n"
     ]
    }
   ],
   "source": [
    "# old weights\n",
    "\n",
    "# weight_for_0 = 1.0 / counts[0]\n",
    "\n",
    "# weight_for_1 = 1.0 / counts[1]\n",
    "\n",
    "\n",
    "#new weights\n",
    "\n",
    "weight_for_0 = float(counts[1]) / len(y_train)\n",
    "\n",
    "weight_for_1 = 1 - (float(counts[1]) / len(y_train))\n",
    "\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "700f978d-cc6f-484a-864f-dcb9e703eed4",
   "metadata": {
    "papermill": {
     "duration": 1.029155,
     "end_time": "2025-03-05T19:05:06.790734",
     "exception": false,
     "start_time": "2025-03-05T19:05:05.761579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "\n",
    "keras.metrics.BinaryCrossentropy(name='cross entropy'),\n",
    "\n",
    "keras.metrics.MeanSquaredError(name='mean_squared_error'),\n",
    "\n",
    "keras.metrics.RootMeanSquaredError(name='root_mean_squared_error'),\n",
    "\n",
    "keras.metrics.TruePositives(name='tp'),\n",
    "\n",
    "keras.metrics.FalsePositives(name='fp'),\n",
    "\n",
    "keras.metrics.TrueNegatives(name='tn'),\n",
    "\n",
    "keras.metrics.FalseNegatives(name='fn'),\n",
    "\n",
    "keras.metrics.BinaryAccuracy(name='binary_accuracy'),\n",
    "\n",
    "keras.metrics.F1Score(threshold=0.5, name='f1_score'),\n",
    "\n",
    "keras.metrics.Precision(name='precision'),\n",
    "\n",
    "keras.metrics.Recall(name='recall'),\n",
    "\n",
    "keras.metrics.AUC(name='auc'),\n",
    "\n",
    "keras.metrics.AUC(name='prc', curve='PR'),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9900636-a8fb-4b46-9282-66643c95cdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6bc9aab7-2558-4304-a6c7-5809d60e8437",
   "metadata": {
    "papermill": {
     "duration": 0.030448,
     "end_time": "2025-03-05T19:05:06.829775",
     "exception": false,
     "start_time": "2025-03-05T19:05:06.799327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyHyperModel(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = keras.Sequential()\n",
    "\n",
    "        model.add(keras.Input(shape=(32, 32, number_of_features)))\n",
    "\n",
    "        # Data augmentation\n",
    "        model.add(layers.RandomFlip(\"horizontal_and_vertical\"))\n",
    "        model.add(layers.RandomRotation(factor=(-0.5, 0.5)))\n",
    "\n",
    "        # CNN layers (normal Conv2D, no BatchNorm)\n",
    "        featmaps1 = hp.Int('units_1', min_value=10, max_value=60)\n",
    "        featmaps2 = hp.Int('units_2', min_value=10, max_value=64)\n",
    "        featmaps3 = hp.Int('units_3', min_value=10, max_value=128)\n",
    "        featmaps4 = hp.Int('units_4', min_value=10, max_value=80)\n",
    "        learning_rate = hp.Float('lr', min_value=1e-5, max_value=1e-2, sampling=\"log\")\n",
    "        act_func = hp.Choice('activation', [\"relu\", \"tanh\", \"selu\", \"sigmoid\", \"relu6\", \"silu\", \"gelu\"])\n",
    "\n",
    "        model.add(layers.Conv2D(featmaps1, 3, strides=1, padding=\"same\", activation=pick_loss(act_func)))\n",
    "        model.add(layers.MaxPooling2D(2))\n",
    "        model.add(layers.Dropout(0.2))\n",
    "\n",
    "        model.add(layers.Conv2D(featmaps2, 3, strides=1, padding=\"same\", activation=pick_loss(act_func)))\n",
    "        model.add(layers.MaxPooling2D(2))\n",
    "        model.add(layers.Dropout(0.2))\n",
    "\n",
    "        model.add(layers.Conv2D(featmaps3, 3, strides=1, padding=\"same\", activation=pick_loss(act_func)))\n",
    "        model.add(layers.MaxPooling2D(2))\n",
    "        model.add(layers.Dropout(0.2))\n",
    "\n",
    "        model.add(layers.GlobalMaxPooling2D())\n",
    "        model.add(layers.Dense(featmaps4))\n",
    "        model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "        \n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=learning_rate,\n",
    "            decay_steps=2000,\n",
    "            decay_rate=0.9,\n",
    "            staircase=True\n",
    "        )\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "        # Compile with Fixed Focal Loss (gamma=2.0, alpha=0.25)\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=focal_loss(gamma=2.0, alpha=0.25),\n",
    "            metrics=METRICS\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e68e967f-623f-4332-a38c-57359b8121c3",
   "metadata": {
    "papermill": {
     "duration": 0.018907,
     "end_time": "2025-03-05T19:05:06.856235",
     "exception": false,
     "start_time": "2025-03-05T19:05:06.837328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit(self, hp, model, *args, **kwargs):\n",
    "    batchsizenum = hp.Int('batch_size', min_value=10, max_value=60, step=5, sampling=\"linear\")\n",
    "\n",
    "    print({k: hp.get(k) if hp.is_active(k) else None for k in hp._hps})\n",
    "\n",
    "    return model.fit(\n",
    "        *args,\n",
    "        batch_size=batchsizenum,\n",
    "        # normally we might use early stopping, but not needed since\n",
    "        # callbacks saves checkpoints of the model during trials\n",
    "        # callbacks=keras.callbacks.EarlyStopping('val_loss', patience=5),\n",
    "        validation_split=0.1,\n",
    "        shuffle=True,\n",
    "        class_weight=class_weight,\n",
    "        **kwargs,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4b0947fd-6eb0-4f20-8b0f-2f5d11ab5b3b",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.379052,
     "end_time": "2025-03-05T19:05:07.243192",
     "exception": false,
     "start_time": "2025-03-05T19:05:06.864140",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "Parameters",
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 6\n",
      "units_1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 10, 'max_value': 60, 'step': 1, 'sampling': 'linear'}\n",
      "units_2 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 10, 'max_value': 64, 'step': 1, 'sampling': 'linear'}\n",
      "units_3 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 10, 'max_value': 128, 'step': 1, 'sampling': 'linear'}\n",
      "units_4 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 10, 'max_value': 80, 'step': 1, 'sampling': 'linear'}\n",
      "lr (Float)\n",
      "{'default': 1e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n",
      "activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh', 'selu', 'sigmoid', 'relu6', 'silu', 'gelu'], 'ordered': False}\n"
     ]
    }
   ],
   "source": [
    "# make the tuner object\n",
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    hypermodel=MyHyperModel(),\n",
    "    objective=keras_tuner.Objective(\"val_f1_score\", direction=\"max\"),\n",
    "    max_trials=150,\n",
    "    project_name=tuner_project_name,\n",
    "    alpha=0.0001,\n",
    "    beta=2.6,\n",
    "    seed=123,\n",
    "    tune_new_entries=True,\n",
    "    allow_new_entries=True,\n",
    "    max_retries_per_trial=1,\n",
    "    max_consecutive_failed_trials=3,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# summary\n",
    "tuner.search_space_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ff5a6396-3b45-4ae6-91ad-62b07260777e",
   "metadata": {
    "papermill": {
     "duration": 6055.188829,
     "end_time": "2025-03-05T20:46:02.439902",
     "exception": false,
     "start_time": "2025-03-05T19:05:07.251073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 32 Complete [00h 00m 26s]\n",
      "val_f1_score: 0.6153846383094788\n",
      "\n",
      "Best val_f1_score So Far: 0.800000011920929\n",
      "Total elapsed time: 00h 13m 07s\n",
      "\n",
      "Search: Running Trial #33\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "35                |36                |units_1\n",
      "39                |39                |units_2\n",
      "10                |10                |units_3\n",
      "55                |55                |units_4\n",
      "0.0032817         |0.002817          |lr\n",
      "silu              |gelu              |activation\n",
      "\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 2s 40ms/step - loss: 0.0561 - cross entropy: 0.4356 - mean_squared_error: 0.1378 - root_mean_squared_error: 0.3712 - tp: 4.0000 - fp: 15.0000 - tn: 454.0000 - fn: 87.0000 - binary_accuracy: 0.8179 - f1_score: 0.0727 - precision: 0.2105 - recall: 0.0440 - auc: 0.6971 - prc: 0.2643 - val_loss: 0.0442 - val_cross entropy: 0.4462 - val_mean_squared_error: 0.1363 - val_root_mean_squared_error: 0.3692 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6567 - val_prc: 0.2641\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0378 - cross entropy: 0.3950 - mean_squared_error: 0.1229 - root_mean_squared_error: 0.3506 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 419.0000 - fn: 85.0000 - binary_accuracy: 0.8313 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8022 - prc: 0.3772 - val_loss: 0.0315 - val_cross entropy: 0.3601 - val_mean_squared_error: 0.1028 - val_root_mean_squared_error: 0.3206 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7017 - val_prc: 0.3812\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0348 - cross entropy: 0.3699 - mean_squared_error: 0.1161 - root_mean_squared_error: 0.3407 - tp: 0.0000e+00 - fp: 1.0000 - tn: 418.0000 - fn: 85.0000 - binary_accuracy: 0.8294 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8340 - prc: 0.4390 - val_loss: 0.0345 - val_cross entropy: 0.3909 - val_mean_squared_error: 0.1123 - val_root_mean_squared_error: 0.3352 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6567 - val_prc: 0.4452\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0326 - cross entropy: 0.3668 - mean_squared_error: 0.1132 - root_mean_squared_error: 0.3365 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 419.0000 - fn: 85.0000 - binary_accuracy: 0.8313 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8655 - prc: 0.5133 - val_loss: 0.0346 - val_cross entropy: 0.3846 - val_mean_squared_error: 0.1113 - val_root_mean_squared_error: 0.3336 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6667 - val_prc: 0.3596\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.0319 - cross entropy: 0.3578 - mean_squared_error: 0.1099 - root_mean_squared_error: 0.3314 - tp: 2.0000 - fp: 1.0000 - tn: 418.0000 - fn: 83.0000 - binary_accuracy: 0.8333 - f1_score: 0.0455 - precision: 0.6667 - recall: 0.0235 - auc: 0.8744 - prc: 0.5328 - val_loss: 0.0357 - val_cross entropy: 0.3938 - val_mean_squared_error: 0.1140 - val_root_mean_squared_error: 0.3377 - val_tp: 1.0000 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 5.0000 - val_binary_accuracy: 0.9107 - val_f1_score: 0.2857 - val_precision: 1.0000 - val_recall: 0.1667 - val_auc: 0.6867 - val_prc: 0.4930\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0313 - cross entropy: 0.3372 - mean_squared_error: 0.1032 - root_mean_squared_error: 0.3213 - tp: 18.0000 - fp: 5.0000 - tn: 414.0000 - fn: 67.0000 - binary_accuracy: 0.8571 - f1_score: 0.3333 - precision: 0.7826 - recall: 0.2118 - auc: 0.8871 - prc: 0.5883 - val_loss: 0.0339 - val_cross entropy: 0.4040 - val_mean_squared_error: 0.1167 - val_root_mean_squared_error: 0.3417 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7067 - val_prc: 0.4734\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0297 - cross entropy: 0.3377 - mean_squared_error: 0.1024 - root_mean_squared_error: 0.3199 - tp: 8.0000 - fp: 6.0000 - tn: 413.0000 - fn: 77.0000 - binary_accuracy: 0.8353 - f1_score: 0.1616 - precision: 0.5714 - recall: 0.0941 - auc: 0.8993 - prc: 0.5829 - val_loss: 0.0348 - val_cross entropy: 0.3835 - val_mean_squared_error: 0.1094 - val_root_mean_squared_error: 0.3308 - val_tp: 1.0000 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 5.0000 - val_binary_accuracy: 0.9107 - val_f1_score: 0.2857 - val_precision: 1.0000 - val_recall: 0.1667 - val_auc: 0.6933 - val_prc: 0.4937\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0314 - cross entropy: 0.3549 - mean_squared_error: 0.1078 - root_mean_squared_error: 0.3283 - tp: 3.0000 - fp: 0.0000e+00 - tn: 419.0000 - fn: 82.0000 - binary_accuracy: 0.8373 - f1_score: 0.0682 - precision: 1.0000 - recall: 0.0353 - auc: 0.8860 - prc: 0.6182 - val_loss: 0.0374 - val_cross entropy: 0.4098 - val_mean_squared_error: 0.1192 - val_root_mean_squared_error: 0.3452 - val_tp: 1.0000 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 5.0000 - val_binary_accuracy: 0.9107 - val_f1_score: 0.2857 - val_precision: 1.0000 - val_recall: 0.1667 - val_auc: 0.6817 - val_prc: 0.4672\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0307 - cross entropy: 0.3169 - mean_squared_error: 0.0971 - root_mean_squared_error: 0.3116 - tp: 20.0000 - fp: 11.0000 - tn: 408.0000 - fn: 65.0000 - binary_accuracy: 0.8492 - f1_score: 0.3448 - precision: 0.6452 - recall: 0.2353 - auc: 0.9043 - prc: 0.6019 - val_loss: 0.0301 - val_cross entropy: 0.3699 - val_mean_squared_error: 0.1039 - val_root_mean_squared_error: 0.3223 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7233 - val_prc: 0.5640\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0336 - cross entropy: 0.3712 - mean_squared_error: 0.1163 - root_mean_squared_error: 0.3410 - tp: 0.0000e+00 - fp: 0.0000e+00 - tn: 419.0000 - fn: 85.0000 - binary_accuracy: 0.8313 - f1_score: 0.0000e+00 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.8420 - prc: 0.4787 - val_loss: 0.0397 - val_cross entropy: 0.3341 - val_mean_squared_error: 0.0959 - val_root_mean_squared_error: 0.3096 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.7083 - val_prc: 0.2828\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.0316 - cross entropy: 0.3362 - mean_squared_error: 0.1056 - root_mean_squared_error: 0.3249 - tp: 6.0000 - fp: 3.0000 - tn: 416.0000 - fn: 79.0000 - binary_accuracy: 0.8373 - f1_score: 0.1277 - precision: 0.6667 - recall: 0.0706 - auc: 0.8748 - prc: 0.5371 - val_loss: 0.0366 - val_cross entropy: 0.4273 - val_mean_squared_error: 0.1257 - val_root_mean_squared_error: 0.3546 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.6617 - val_prc: 0.4346\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0286 - cross entropy: 0.3354 - mean_squared_error: 0.1001 - root_mean_squared_error: 0.3164 - tp: 21.0000 - fp: 7.0000 - tn: 412.0000 - fn: 64.0000 - binary_accuracy: 0.8591 - f1_score: 0.3717 - precision: 0.7500 - recall: 0.2471 - auc: 0.9110 - prc: 0.6783 - val_loss: 0.0413 - val_cross entropy: 0.3796 - val_mean_squared_error: 0.1053 - val_root_mean_squared_error: 0.3245 - val_tp: 1.0000 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 5.0000 - val_binary_accuracy: 0.9107 - val_f1_score: 0.2857 - val_precision: 1.0000 - val_recall: 0.1667 - val_auc: 0.6650 - val_prc: 0.3418\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 0.0272 - cross entropy: 0.3204 - mean_squared_error: 0.0961 - root_mean_squared_error: 0.3100 - tp: 16.0000 - fp: 4.0000 - tn: 415.0000 - fn: 69.0000 - binary_accuracy: 0.8552 - f1_score: 0.3048 - precision: 0.8000 - recall: 0.1882 - auc: 0.9115 - prc: 0.7085 - val_loss: 0.0447 - val_cross entropy: 0.4077 - val_mean_squared_error: 0.1184 - val_root_mean_squared_error: 0.3441 - val_tp: 2.0000 - val_fp: 2.0000 - val_tn: 48.0000 - val_fn: 4.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.4000 - val_precision: 0.5000 - val_recall: 0.3333 - val_auc: 0.6633 - val_prc: 0.4492\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0259 - cross entropy: 0.2969 - mean_squared_error: 0.0874 - root_mean_squared_error: 0.2956 - tp: 36.0000 - fp: 8.0000 - tn: 411.0000 - fn: 49.0000 - binary_accuracy: 0.8869 - f1_score: 0.5581 - precision: 0.8182 - recall: 0.4235 - auc: 0.9228 - prc: 0.7425 - val_loss: 0.0564 - val_cross entropy: 0.3921 - val_mean_squared_error: 0.1035 - val_root_mean_squared_error: 0.3217 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 6.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5633 - val_prc: 0.1563\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 19ms/step - loss: 0.0278 - cross entropy: 0.3124 - mean_squared_error: 0.0951 - root_mean_squared_error: 0.3084 - tp: 23.0000 - fp: 4.0000 - tn: 415.0000 - fn: 62.0000 - binary_accuracy: 0.8690 - f1_score: 0.4107 - precision: 0.8519 - recall: 0.2706 - auc: 0.9049 - prc: 0.6904 - val_loss: 0.0363 - val_cross entropy: 0.3979 - val_mean_squared_error: 0.1141 - val_root_mean_squared_error: 0.3377 - val_tp: 2.0000 - val_fp: 1.0000 - val_tn: 49.0000 - val_fn: 4.0000 - val_binary_accuracy: 0.9107 - val_f1_score: 0.4444 - val_precision: 0.6667 - val_recall: 0.3333 - val_auc: 0.6783 - val_prc: 0.4550\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0227 - cross entropy: 0.2801 - mean_squared_error: 0.0804 - root_mean_squared_error: 0.2835 - tp: 42.0000 - fp: 8.0000 - tn: 411.0000 - fn: 43.0000 - binary_accuracy: 0.8988 - f1_score: 0.6222 - precision: 0.8400 - recall: 0.4941 - auc: 0.9429 - prc: 0.7944 - val_loss: 0.0304 - val_cross entropy: 0.3294 - val_mean_squared_error: 0.0904 - val_root_mean_squared_error: 0.3006 - val_tp: 1.0000 - val_fp: 0.0000e+00 - val_tn: 50.0000 - val_fn: 5.0000 - val_binary_accuracy: 0.9107 - val_f1_score: 0.2857 - val_precision: 1.0000 - val_recall: 0.1667 - val_auc: 0.7300 - val_prc: 0.4797\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0256 - cross entropy: 0.2892 - mean_squared_error: 0.0859 - root_mean_squared_error: 0.2930 - tp: 28.0000 - fp: 5.0000 - tn: 414.0000 - fn: 57.0000 - binary_accuracy: 0.8770 - f1_score: 0.4746 - precision: 0.8485 - recall: 0.3294 - auc: 0.9197 - prc: 0.7491 - val_loss: 0.0383 - val_cross entropy: 0.3698 - val_mean_squared_error: 0.1063 - val_root_mean_squared_error: 0.3260 - val_tp: 1.0000 - val_fp: 1.0000 - val_tn: 49.0000 - val_fn: 5.0000 - val_binary_accuracy: 0.8929 - val_f1_score: 0.2500 - val_precision: 0.5000 - val_recall: 0.1667 - val_auc: 0.6600 - val_prc: 0.3593\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0229 - cross entropy: 0.2818 - mean_squared_error: 0.0796 - root_mean_squared_error: 0.2822 - tp: 47.0000 - fp: 8.0000 - tn: 411.0000 - fn: 38.0000 - binary_accuracy: 0.9087 - f1_score: 0.6714 - precision: 0.8545 - recall: 0.5529 - auc: 0.9461 - prc: 0.7889 - val_loss: 0.0463 - val_cross entropy: 0.3753 - val_mean_squared_error: 0.1113 - val_root_mean_squared_error: 0.3336 - val_tp: 1.0000 - val_fp: 3.0000 - val_tn: 47.0000 - val_fn: 5.0000 - val_binary_accuracy: 0.8571 - val_f1_score: 0.2000 - val_precision: 0.2500 - val_recall: 0.1667 - val_auc: 0.6583 - val_prc: 0.3048\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.0227 - cross entropy: 0.2744 - mean_squared_error: 0.0806 - root_mean_squared_error: 0.2839 - tp: 34.0000 - fp: 2.0000 - tn: 417.0000 - fn: 51.0000 - binary_accuracy: 0.8948 - f1_score: 0.5620 - precision: 0.9444 - recall: 0.4000 - auc: 0.9427 - prc: 0.8061 - val_loss: 0.0462 - val_cross entropy: 0.4286 - val_mean_squared_error: 0.1291 - val_root_mean_squared_error: 0.3593 - val_tp: 2.0000 - val_fp: 4.0000 - val_tn: 46.0000 - val_fn: 4.0000 - val_binary_accuracy: 0.8571 - val_f1_score: 0.3333 - val_precision: 0.3333 - val_recall: 0.3333 - val_auc: 0.7483 - val_prc: 0.4268\n",
      "Epoch 20/100\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 0.0125 - cross entropy: 0.2522 - mean_squared_error: 0.0586 - root_mean_squared_error: 0.2421 - tp: 5.0000 - fp: 0.0000e+00 - tn: 27.0000 - fn: 0.0000e+00 - binary_accuracy: 1.0000 - f1_score: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - prc: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Begin the search\u001b[39;00m\n\u001b[1;32m      2\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_train_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_train_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Query the tuner object to grab the best models\u001b[39;00m\n\u001b[1;32m     12\u001b[0m models \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_models(num_models\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras_tuner/src/engine/base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras_tuner/src/engine/base_tuner.py:274\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m         trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m trial_module\u001b[38;5;241m.\u001b[39mTrialStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras_tuner/src/engine/base_tuner.py:239\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[0;32m--> 239\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mexists(\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    242\u001b[0m     ):\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe use case of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    255\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras_tuner/src/engine/tuner.py:314\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[1;32m    313\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[0;32m--> 314\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_and_fit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcopied_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras_tuner/src/engine/tuner.py:233\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[1;32m    232\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[0;32m--> 233\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhypermodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_backend():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras_tuner/src/engine/hypermodel.py:149\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    262\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1494\u001b[0m   )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Begin the search\n",
    "EPOCHS = 100\n",
    "\n",
    "tuner.search(\n",
    "    input_train_scaled,\n",
    "    label_train_scaled,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "# Query the tuner object to grab the best models\n",
    "models = tuner.get_best_models(num_models=5)\n",
    "\n",
    "# Here is the best model from the tuner\n",
    "best_model = models[0]\n",
    "print(best_model.summary())\n",
    "\n",
    "# Get the top hyperparameters.\n",
    "best_hps = tuner.get_best_hyperparameters()\n",
    "\n",
    "# Build the model with the best hyperparameters.\n",
    "# Assuming your MyHyperModel has a build method that takes a hyperparameter object.\n",
    "model = MyHyperModel().build(best_hps[0])\n",
    "\n",
    "# Save the best model (architecture and weights)\n",
    "\n",
    "save_dir = \"/glade/derecho/scratch/rmandava/AEW_time_location_files/models\"\n",
    "model_save_path = os.path.join(save_dir, model_save_name)\n",
    "\n",
    "model.save(model_save_path)\n",
    "\n",
    "\n",
    "# Retrain using \"best\" model hyperparameters\n",
    "history = model.fit(\n",
    "    input_train_scaled,\n",
    "    label_train_scaled,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.1,\n",
    "    batch_size= 32,\n",
    "    # callbacks=keras.callbacks.EarlyStopping('val_loss', patience=3),\n",
    "    shuffle=True,\n",
    "    class_weight=class_weight,  # Ensure 'class_weight' is defined in your scope\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "results = model.evaluate(input_test_scaled, label_test_scaled, batch_size=label_test_scaled.shape[0])\n",
    "print(results)\n",
    "\n",
    "# Generate predictions (probabilitiesthe output of the last layer)\n",
    "predictions = model.predict(input_test_scaled)\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "\n",
    "# Compute confusion matrix elements\n",
    "tn, fp, fn, tp = sklearn.metrics.confusion_matrix(\n",
    "    label_test_scaled, np.round(predictions)\n",
    ").ravel()\n",
    "print(\"tn:\", tn)\n",
    "print(\"fp:\", fp)\n",
    "print(\"fn:\", fn)\n",
    "print(\"tp:\", tp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8883c1-4450-444a-88a5-18ed3d94d588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Ensemble top K models  \n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# 1) Grab the top K hyperparameters\n",
    "top_hps = tuner.get_best_hyperparameters(num_trials=5)\n",
    "\n",
    "# 2) Rebuild each best model\n",
    "models = [ MyHyperModel().build(hp) for hp in top_hps ]\n",
    "# (Or, if you saved weights per trial, load them here onto each model.)\n",
    "\n",
    "# 3) Run each model on the test set\n",
    "all_preds = np.stack([m.predict(input_test_scaled).flatten() for m in models], axis=0)\n",
    "\n",
    "# 4) Average their probabilities & threshold\n",
    "ensemble_probs = all_preds.mean(axis=0)\n",
    "ensemble_preds = (ensemble_probs >= 0.5).astype(int)\n",
    "\n",
    "# 5) Compute & print ensemble F1\n",
    "print(\"Ensemble F1:\", f1_score(label_test_scaled, ensemble_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d463e2-12e7-474b-95c1-ae20c22a093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = np.round(predictions.flatten())\n",
    "true_labels = label_test_scaled.flatten()\n",
    "false_neg_idx = np.where((true_labels == 1) & (pred_labels == 0))[0]\n",
    "false_pos_idx = np.where((true_labels == 0) & (pred_labels == 1))[0]\n",
    "true_pos_idx = np.where((true_labels == 1) & (pred_labels == 1))[0]\n",
    "true_neg_idx = np.where((true_labels == 0) & (pred_labels == 0))[0]\n",
    "\n",
    "print(\"Number of false negatives:\", len(false_neg_idx))\n",
    "print(\"Number of false positives:\", len(false_pos_idx))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ddefd-a847-4a5c-a028-ca25e1e1d87c",
   "metadata": {
    "papermill": {
     "duration": 0.042059,
     "end_time": "2025-03-05T20:46:02.526276",
     "exception": false,
     "start_time": "2025-03-05T20:46:02.484217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming lat_test and lon_test are arrays with shape (num_samples, height, width)\n",
    "#central_lat = lat_test[:, lat_test.shape[1]//2, lat_test.shape[2]//2]\n",
    "#central_lon = lon_test[:, lon_test.shape[1]//2, lon_test.shape[2]//2]\n",
    "\n",
    "#lats_false_neg = central_lat[false_neg_idx]\n",
    "#lons_false_neg = central_lon[false_neg_idx]\n",
    "\n",
    "#lats_false_pos = central_lat[false_pos_idx]\n",
    "#lons_false_pos = central_lon[false_pos_idx]\n",
    "# Assuming lat_test and lon_test are 1D arrays with one coordinate per sample\n",
    "lat_false_neg = lat_test[false_neg_idx]\n",
    "lon_false_neg = lon_test[false_neg_idx]\n",
    "\n",
    "lat_false_pos = lat_test[false_pos_idx]\n",
    "lon_false_pos = lon_test[false_pos_idx]\n",
    "\n",
    "lat_true_pos = lat_test[true_pos_idx]\n",
    "lon_true_pos = lon_test[true_pos_idx]\n",
    "\n",
    "lat_true_neg = lat_test[true_neg_idx]\n",
    "lon_true_neg = lon_test[true_neg_idx]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798796b5-b155-48c8-bd46-4c1a1f5fedba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique latitudes in test set:\", np.unique(lat_test))\n",
    "print(\"Unique longitudes in test set:\", np.unique(lon_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0f4581-4248-48f1-bd51-5d4bb476f030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(lon_false_neg, lat_false_neg, marker='x', color='red', label='False Negatives')\n",
    "plt.scatter(lon_false_pos, lat_false_pos, marker='o', color='blue', label='False Positives')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Geographic Distribution of Misclassifications')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f3eb33-d853-48b3-8416-46e48cd99444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512e75d1-8ea6-4a7e-90c9-aaf6ad6d5a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "\n",
    "\n",
    "lon_min, lon_max = np.min(lon_test)-15, np.max(lon_test)+15\n",
    "lat_min, lat_max = np.min(lat_test)-15, np.max(lat_test)+15\n",
    "ax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Add map features\n",
    "ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':', linewidth=0.5)\n",
    "ax.add_feature(cfeature.LAND, edgecolor='black', facecolor='lightgray')\n",
    "ax.add_feature(cfeature.OCEAN, facecolor='lightblue')\n",
    "\n",
    "# Plot misclassified points:\n",
    "ax.scatter(lon_false_neg, lat_false_neg, color='red', marker='x', s=100,\n",
    "           transform=ccrs.PlateCarree(), label='False Negatives')\n",
    "ax.scatter(lon_false_pos, lat_false_pos, color='blue', marker='o', s=100,\n",
    "           transform=ccrs.PlateCarree(), label='False Positives')\n",
    "\n",
    "plt.title(\"Misclassified Test Samples on Map\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4561912c-9788-442a-a527-f125871dbce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Set map extent (adjust margins as desired)\n",
    "lon_min, lon_max = np.min(lon_test) - 10, np.max(lon_test) + 10\n",
    "lat_min, lat_max = np.min(lat_test) - 10, np.max(lat_test) + 10\n",
    "ax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Add map features\n",
    "ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':', linewidth=0.5)\n",
    "ax.add_feature(cfeature.LAND, facecolor='lightgray', edgecolor='black')\n",
    "ax.add_feature(cfeature.OCEAN, facecolor='lightblue')\n",
    "# Plot misclassified points\n",
    "ax.scatter(lon_false_neg, lat_false_neg, color='red', marker='x', s=100,\n",
    "           transform=ccrs.PlateCarree(), label='False Negatives')\n",
    "ax.scatter(lon_false_pos, lat_false_pos, color='blue', marker='o', s=100,\n",
    "           transform=ccrs.PlateCarree(), label='False Positives')\n",
    "\n",
    "\n",
    "\n",
    "# Plot correctly classified points\n",
    "ax.scatter(lon_true_pos, lat_true_pos, color='green', marker='^', s=100,\n",
    "           transform=ccrs.PlateCarree(), label='True Positives')\n",
    "ax.scatter(lon_true_neg, lat_true_neg, color='orange', marker='s', s=100,\n",
    "           transform=ccrs.PlateCarree(), label='True Negatives')\n",
    "\n",
    "plt.title(\"Test Set Classification Results on Map\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe393f-a6c9-440d-b648-78010a1d0ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total samples:\", len(sample_lat))\n",
    "print(\"Unique latitudes:\", len(np.unique(sample_lat)))\n",
    "print(\"Unique longitudes:\", len(np.unique(sample_lon)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e932dda-37e1-4158-8655-7d2a041a1a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First 10 latitudes:\", sample_lat[:10])\n",
    "print(\"First 10 longitudes:\", sample_lon[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe686778-d179-4dec-8b2e-472700d70e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(sample_lon, sample_lat, c='green', marker='o')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Per-Sample Weighted Centroid Coordinates')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5563a928-7d7f-402d-94e1-63dd0cff0982",
   "metadata": {
    "papermill": {
     "duration": 13.848113,
     "end_time": "2025-03-05T20:46:16.415723",
     "exception": false,
     "start_time": "2025-03-05T20:46:02.567610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Permutation Importance ---\n",
    "# Evaluate baseline performance using your loss metric (here, the custom f1_loss_sigmoid)\n",
    "# model.metrics_names gives a list where index 0 is 'loss'\n",
    "baseline_results = model.evaluate(input_test_scaled, label_test_scaled,\n",
    "                                  batch_size=label_test_scaled.shape[0],\n",
    "                                  verbose=0)\n",
    "baseline_loss = baseline_results[model.metrics_names.index('loss')]\n",
    "print(\"Baseline loss:\", baseline_loss)\n",
    "\n",
    "# Set the number of repetitions for averaging\n",
    "n_repeats = 5\n",
    "n_features = input_test_scaled.shape[-1]\n",
    "permutation_importances = np.zeros(n_features)\n",
    "\n",
    "# Loop over each feature (channel)\n",
    "for feature_idx in range(n_features):\n",
    "    permuted_losses = []\n",
    "    for _ in range(n_repeats):\n",
    "        # Copy the test set to avoid modifying the original\n",
    "        X_permuted = np.copy(input_test_scaled)\n",
    "        # Permute the values of the selected feature across samples\n",
    "        perm = np.random.permutation(X_permuted.shape[0])\n",
    "        X_permuted[:, :, :, feature_idx] = X_permuted[perm, :, :, feature_idx]\n",
    "        \n",
    "        # Evaluate the model on the permuted test set\n",
    "        permuted_results = model.evaluate(X_permuted, label_test_scaled,\n",
    "                                          batch_size=label_test_scaled.shape[0],\n",
    "                                          verbose=0)\n",
    "        permuted_loss = permuted_results[model.metrics_names.index('loss')]\n",
    "        permuted_losses.append(permuted_loss)\n",
    "    \n",
    "    avg_permuted_loss = np.mean(permuted_losses)\n",
    "    # The difference between the permuted loss and baseline loss indicates feature importance:\n",
    "    # A larger increase means the feature is more important.\n",
    "    permutation_importances[feature_idx] = avg_permuted_loss - baseline_loss\n",
    "    print(f\"Feature {feature_idx} - Increase in Loss: {permutation_importances[feature_idx]}\")\n",
    "\n",
    "print(\"Permutation Importances (increase in loss) for all features:\", permutation_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23e4f15-e79e-4040-b328-2096906415f9",
   "metadata": {
    "papermill": {
     "duration": 0.467125,
     "end_time": "2025-03-05T20:46:16.925109",
     "exception": false,
     "start_time": "2025-03-05T20:46:16.457984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Suppose permutation_importances is the numpy array you printed:\n",
    "# e.g., [0.0820, 0.0824, 0.0815, 0.0864, 0.0121, ...]\n",
    "\n",
    "feature_indices = np.arange(len(permutation_importances))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(feature_indices, permutation_importances)\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Increase in Loss (Permutation Importance)\")\n",
    "plt.title(\"Permutation Importance by Feature\")\n",
    "plt.xticks(feature_indices, [f\"F{i}\" for i in feature_indices], rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1765f8b0-69ed-4917-82f7-f15ef12cc837",
   "metadata": {
    "papermill": {
     "duration": 0.043518,
     "end_time": "2025-03-05T20:46:17.012058",
     "exception": false,
     "start_time": "2025-03-05T20:46:16.968540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_index = 0\n",
    "sample_input = input_test_scaled[sample_index:sample_index+1]\n",
    "\n",
    "# Compute the saliency map for the selected test sample\n",
    "saliency_map = compute_saliency_map(model, sample_input)\n",
    "\n",
    "# Plot the saliency map\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(saliency_map, cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.title(f\"Saliency Map for Sample Index {sample_index}\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa54c93-f08a-46e2-a610-95424eb79806",
   "metadata": {
    "papermill": {
     "duration": 0.042628,
     "end_time": "2025-03-05T20:46:17.098037",
     "exception": false,
     "start_time": "2025-03-05T20:46:17.055409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_index = 1\n",
    "sample_input = input_test_scaled[sample_index:sample_index+1]\n",
    "\n",
    "# Compute saliency maps per channel for the selected input sample\n",
    "saliency_maps, channel_importance = compute_saliency_per_channel(model, sample_input)\n",
    "\n",
    "# Plot the saliency maps for each channel\n",
    "num_channels = saliency_maps.shape[-1]\n",
    "cols = 5  # Set number of columns for plotting\n",
    "rows = int(np.ceil(num_channels / cols))\n",
    "plt.figure(figsize=(15, rows * 3))\n",
    "for c in range(num_channels):\n",
    "    plt.subplot(rows, cols, c + 1)\n",
    "    plt.imshow(saliency_maps[:, :, c], cmap='hot')\n",
    "    plt.title(f'Channel {c}\\nMean: {channel_importance[c]:.4f}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print aggregated channel importance values\n",
    "print(\"Channel importance (mean saliency per channel):\")\n",
    "for c, imp in enumerate(channel_importance):\n",
    "    print(f\"Channel {c}: {imp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec3979-63a3-4211-9475-3445ad530009",
   "metadata": {
    "papermill": {
     "duration": 0.043201,
     "end_time": "2025-03-05T20:46:17.184585",
     "exception": false,
     "start_time": "2025-03-05T20:46:17.141384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Compute persample, perchannel saliency importances\n",
    "all_imps = []\n",
    "for x in input_test_scaled:             # each x has shape (32,32,channels)\n",
    "    _, imp = compute_saliency_per_channel(best_model, x[np.newaxis,...])\n",
    "    all_imps.append(imp)               # imp.shape == (channels,)\n",
    "all_imps = np.stack(all_imps)          # shape (N_samples, channels)\n",
    "\n",
    "# 2) Average importance across samples\n",
    "mean_imp = all_imps.mean(axis=0)       # shape (channels,)\n",
    "\n",
    "# 3) Build a DataFrame mapping featureplevelimportance\n",
    "df = pd.DataFrame({\n",
    "    \"feature\": var_list,\n",
    "    \"plevel\": plevel_list,\n",
    "    \"importance\": mean_imp\n",
    "})\n",
    "\n",
    "# 4) Group by pressure level and plot\n",
    "grouped = df.groupby(\"plevel\")[\"importance\"].mean().reset_index()\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(grouped[\"plevel\"].astype(str), grouped[\"importance\"])\n",
    "plt.xlabel(\"Pressure level (hPa or False=surface)\")\n",
    "plt.ylabel(\"Mean saliency importance\")\n",
    "plt.title(\"Average feature importance by pressure level\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f421b06b-75d2-4e23-beb0-dbb5f6c6240f",
   "metadata": {
    "papermill": {
     "duration": 0.041979,
     "end_time": "2025-03-05T20:46:17.268705",
     "exception": false,
     "start_time": "2025-03-05T20:46:17.226726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1) Print out full testset metrics by name\n",
    "test_results = model.evaluate(\n",
    "    input_test_scaled,\n",
    "    label_test_scaled,\n",
    "    batch_size=label_test_scaled.shape[0],\n",
    "    verbose=0\n",
    ")\n",
    "print(dict(zip(model.metrics_names, test_results)))\n",
    "\n",
    "# 2) Peek at the end of your training history for F1 improvements\n",
    "import pandas as pd\n",
    "hist_df = pd.DataFrame(history.history)\n",
    "print(hist_df[['f1_score','val_f1_score']].tail())\n",
    "\n",
    "# 3) (Optional) Plot train vs. val F1 over epochs\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist_df['f1_score'],    label='train F1')\n",
    "plt.plot(hist_df['val_f1_score'],label='val F1')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.title('FocalLoss Training F1 Curves')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b46d58-ad34-4817-a519-c9152372b7d4",
   "metadata": {
    "papermill": {
     "duration": 0.047731,
     "end_time": "2025-03-05T20:46:17.359595",
     "exception": false,
     "start_time": "2025-03-05T20:46:17.311864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace your rebuild logic:\n",
    "# top_hps = tuner.get_best_hyperparameters(num_trials=5)\n",
    "# models  = [MyHyperModel().build(hp) for hp in top_hps]\n",
    "\n",
    "# With this single line:\n",
    "models = tuner.get_best_models(num_models=5)\n",
    "\n",
    "# Then stack and average as before:\n",
    "all_preds     = np.stack([m.predict(input_test_scaled).flatten() for m in models], axis=0)\n",
    "ensemble_probs= all_preds.mean(axis=0)\n",
    "ensemble_preds= (ensemble_probs >= 0.5).astype(int)\n",
    "print(\"Ensemble F1:\", f1_score(label_test_scaled, ensemble_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae3ef17-24f6-4569-8095-2bdee2e67245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d420057-7c8e-4daf-9eda-a4a3b4cf11ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcba43d-dd0b-4f79-b140-bce612bcd0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7035.678234,
   "end_time": "2025-03-05T20:46:20.341478",
   "environment_variables": {},
   "exception": null,
   "input_path": "Hurricaneoriginal(300).ipynb",
   "output_path": "output_notebook_var(second)0.ipynb",
   "parameters": {
    "aew_subset": "12hr_before",
    "model_save_name": "best_model_var0.keras",
    "plevel_list": [
     false,
     false,
     300,
     false,
     false,
     false,
     300,
     300,
     850,
     300,
     850,
     false,
     false,
     false,
     false,
     300,
     850,
     false,
     300,
     850,
     300,
     850,
     300,
     300,
     850
    ],
    "tuner_project_name": "tuner_run(second)_0",
    "var_list": [
     "cape",
     "crr",
     "d",
     "ie",
     "ishf",
     "lsrr",
     "pv",
     "q",
     "q",
     "r",
     "r",
     "sp",
     "sstk",
     "tcw",
     "tcwv",
     "t",
     "t",
     "ttr",
     "u",
     "u",
     "v",
     "v",
     "vo",
     "w",
     "w"
    ]
   },
   "start_time": "2025-03-05T18:49:04.663244",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
